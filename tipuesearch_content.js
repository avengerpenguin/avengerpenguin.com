var tipuesearch = {"pages":[{"title":"  Search Ross Fenning's Digital Garden\n","text":"\n\n\n  Search Ross Fenning's Digital Garden\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoss Fenning's Digital Garden\n\n\nHome\nBlog\nSearch\n\n\n\n\nSearch\n\n\n\n\n\n\n\n\n\n","tags":"","url":"https://avengerpenguin.com/search/index.html"},{"title":"12 Favourite Problems","text":"Personal How can I continue learning all my areas of interest while actually learning some of them in depth? How can I live a life with longer breaks or \"mini-retirements\"? How do I find or build a community of people that share my values, interests and passions? How do I make physical fitness and training a natural part of my life? How do I get better at doing deep work and sharing it? What is the right approach to balance doing deep, meaningful work and ensuring I am taking time out to rest properly and spend time with family? Do I have areas where I can do significant teaching through talks, writing articles and even writing books? How do I incorporate a significant reading habit into my life such that I actually read all the books I have? Tech How do we balance locality and autonomy with the desire to reduce Undifferentiated Heavy Lifting ? Can we restore some of design principles of the World Wide Web (REST, semantics, Linked Data) to APIs and Enterprise Integration and reduce wasted effort on repeated, bespoke integration middlewares? Is it possible to having less than a day between writing code and shipping to production? Can we deploy changes within 10 minutes? Wider How might I do more for the world around me? Can I do anything about injustice, climate change, oppression?","tags":"misc","url":"https://avengerpenguin.com/12-favourite-problems/","loc":"https://avengerpenguin.com/12-favourite-problems/"},{"title":"8values","text":"","tags":"misc","url":"https://avengerpenguin.com/8values/","loc":"https://avengerpenguin.com/8values/"},{"title":"A Model of REST Services","text":"In this paper we explore a theoretic model of REST services. ```python trusted=true from typing import cast, Tuple, Set, NewType, Callable, Dict from rdflib import Graph, URIRef from rdflib.term import Node ClientState = Tuple[URIRef, Graph] ServerState = Tuple[Graph] ```python trusted=true context = { \"dc11\": \"http://purl.org/dc/elements/1.1/\", \"ex\": \"http://example.org/vocab#\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\", \"ex:contains\": { \"@type\": \"@id\" } } g1 = Graph() g1.parse(data=\"\"\" { \"@context\": { \"dc11\": \"http://purl.org/dc/elements/1.1/\", \"ex\": \"http://example.org/vocab#\", \"xsd\": \"http://www.w3.org/2001/XMLSchema#\", \"ex:contains\": { \"@type\": \"@id\" } }, \"@graph\": [ { \"@id\": \"http://example.org/library\", \"@type\": \"ex:Library\", \"ex:contains\": \"http://example.org/library/the-republic\" }, { \"@id\": \"http://example.org/library/the-republic\", \"@type\": \"ex:Book\", \"ex:contains\": \"http://example.org/library/the-republic#introduction\", \"dc11:creator\": \"Plato\", \"dc11:title\": \"The Republic\" }, { \"@id\": \"http://example.org/library/the-republic#introduction\", \"@type\": \"ex:Chapter\", \"dc11:description\": \"An introductory chapter on The Republic.\", \"dc11:title\": \"The Introduction\" } ] } \"\"\", format='json-ld') server: ServerState = (g1, ) def œÉ(g: Graph, uri: URIRef) -> Graph: g2 = Graph() for t in g.query(f\"DESCRIBE <{uri}>\"): g2.add(cast(Tuple[Node, Node, Node], t)) return g2 def GET(ss: ServerState, url: URIRef) -> ClientState: g, = ss return url, œÉ(g, url) def render(cs: ClientState) -> None: uri, g = cs print(str(uri) + \"\\n\\n\" + g.serialize(format='json-ld', indent=2, context=context)) client = GET(server, URIRef(\"http://example.org/library\")) render(client) ```python trusted=true def LO(g: Graph) -> Dict[str, URIRef]: return { p.fragment: o for p, o in g.predicate_objects() if type(o) == URIRef } client = GET(server, LO(client[1])['contains']) render(client) ```python trusted=true from collections import defaultdict class Client: def __init__(self, state: ClientState): self.state = state def __str__(self): uri, g = self.state return str(uri) + \"\\n\\n\" + g.serialize(format='json-ld', indent=2, context=context) def __repr__(self): return str(self) def LO(self): uri, g = self.state links = defaultdict(set) print(uri) for s, p, o in g: #.predicate_objects(subject=uri): print(s, type(s), uri, type(uri), s == uri) print(p, o) if type(o) == URIRef: links[p].add(o) return links Client(GET(server, URIRef(\"http://example.org/library\")).LO()","tags":"notebooks","url":"https://avengerpenguin.com/a-model-of-rest-services/","loc":"https://avengerpenguin.com/a-model-of-rest-services/"},{"title":"Agile Results","text":"I came across Agile Results on a Page and an interview on YouTube with J.D. Meier which talks about an \"Agile Results\" system for getting meaningful results in life. It essentially boils down to focus on a small number of things at a time -- just as with Essentialism -- but specifically: Three wins for the day. Three wins for the week. Three wins for the month. Three wins for the quarter. Three wins for the year. This isn't to be confused with the Agile Methodology but clearly has some overlap with it in that you focus on short term iteration, experiments, etc. with a focus on what you plan to get out of it, rather than just working through a list/backlog.","tags":"Productivity","url":"https://avengerpenguin.com/agile-results/","loc":"https://avengerpenguin.com/agile-results/"},{"title":"Analysis Paralysis","text":"Analysis paralysis is when the fear of either making an error or forgoing a superior solution outweighs the realistic expectation or potential value of success in a decision made in a timely manner.","tags":"Productivity","url":"https://avengerpenguin.com/analysis-paralysis/","loc":"https://avengerpenguin.com/analysis-paralysis/"},{"title":"Bootstrap Continuous Delivery","text":"Things to get that CI/CD feel locally: For written sites, having Obisidian git plugin regularly commit and push Use of \"drafts\" feature in Pelican allows for releasing a proper article later pre-commit pre-commit and pre-push hooks: pre-commit: linting, formatting, etc. pre-push: run tests, stage, verify, publish","tags":"Software Engineering","url":"https://avengerpenguin.com/bootstrap-continuous-delivery/","loc":"https://avengerpenguin.com/bootstrap-continuous-delivery/"},{"title":"Building a Successful Platform","text":"Following from Scaling your business with platform thinking by Thoughtworks , I got a nice list-based set of \"tenets\" for how to build a successful platform inside an organisation. They resonated strongly with my own observations and experience so I have added those to the tenets as laid out by Thoughtworks here. See also Platform Pitfalls and Myths . 1. Clear vision and value hypothesis The first tenet is to have a clear vision of what that platform is to achieve. Additionally, that goal should be aligned with outcomes the overall business wants or focused on delivering value to its customers. 2. Coherent digital platform strategy Following from the clear vision, we should be able to define the value the platform will provide, measure it and seek to maximise it. 3. Product thinking A key approach is to treat a platform as any customer-facing product. That is, we should be able to start with early iterations and allow it to evolve based on emergent need. Where this has not been folllowed we can see: time sunk into used features; poor design decisions causing blockers later on; and potentially a platform nobody actually asked for nor needs (see \"Build it and they will come\" in Platform Pitfalls and Myths ). Once we avoid the \"build it and they will come\" mentality -- as would be the case for any customer-facing product -- we are forced to build a platform that unlocks clear value early on, makes people want to use it (avoiding the mandate pitfall) and is resilient to change over time. A clear change I've seen in successful platforms and tools is the ability to scale back capabilities now available from open source or commercial vendors (where there weren't before). For example, if you provide some CI capabilities that are now available as Jenkins plugins or as features of AWS CodeBuild. This is key in particular with cloud engineering as many things you need to build as an internal platform are likely to become features of cloud vendors later. I've seen similar with services like Github providing capabilities I've previously built in-house. 4. Team Structures The emergence of platforms in an organisation might lead to a need to change ways of working across multiple disciplines and teams. As with Agile transformations, organisations with cross-functional teams benefit the most, particularly when the new ways of working are adopted across all disciplines. Neither Agile nor platform thinking are limited to engineers. 5. Careful with change management It is a significant change to move an organisation to one that adopts platform thinking. Without proper change management to help buy into the platform and adapt workflows to it, expect adoption not to get off the ground and the value of the platform is never unlocked.","tags":"Platform Engineering","url":"https://avengerpenguin.com/building-a-successful-platform/","loc":"https://avengerpenguin.com/building-a-successful-platform/"},{"title":"Commercial Conferences","text":"I use the term \"Commercial Conferences\" for tech conferences set up by a single company (or a small number collaborating) where the agenda is clearly going to focus around their products. This isn't necessarily a bad thing as if their products are solving problems in a space you're interested in, they can put on talks with topics that are useful still. I think there's a couple of \"drag factors\" that almost inevitably detract from the quality not because they are trying to do a bad job, but just because of the nature of the setup. Repetition If a schedule is filled with different people from the same company, almost every talk will come back to their product and how it solves the problem the talk was discussing. Not necessarily bad in isolation, but over the course of a whole conference, it becomes a little repetitive, much like any advert you encounter multiple times. This is especially notable when the \"plug\" comes near the end, since it makes every talk an inevitable journey to the same destination, albeit from different starting points. This leads into the next issue. It's true that even in a more \"general\" conference, most people have been sent to speak for their company and mention their company's products, but at least the worst case here is each talk is an advert for a different product. There's some variety and I can compare things I've seen that day. Lack of Breadth If I attend a talk discussing a problem or an area of interest, I expect a little bit of exploration of the space and different solutions. The talk might be demonstrating one particular solution in some cases, but I'd like to see how it compares to other things. Where a talk is fated to end up pitching a particular product, there are times where I can sense it's come at the expense of exploring other options, the drawbacks to their solution or even how it compares to other things (since as an advert it needs to be careful about comparisons to other products that turn out not to be true). This might seem like it's just an extension of the \"repetition\" issue, but I think it highlights a deeper consequence where talks that are too commercially-driven will have almost an upper limit of how intellectually rich they can be. That is, they can only frame the problem in terms of how their product addresses it, they can't be open to whether the problem doesn't even need solved or if it's a problem we could be avoiding having in the first place. Hard Sell This one is not an inevitable problem as such, but it does come up now and again. It might be something I notice more as a British person attending talks delivered by an American company where some of the sales patter, jokes and other quirks come across as insincere. It's as if it's a bit more impolite to be too \"full on\" with a sales pitch in the UK. I've seen talks where there's lip service to some kind of structure, e.g. a quick \"history of this topic\" that is riddled with oversimplifications, in service of trying to get to the sales pitch. When it's really obvious, at wish they would at least dress up their sales pitch with a genuinely interesting discussion. At one conference, a lot of the speakers seemed quite over-rehearsed and professional to that point of insincerity again. This might be creeping into personal preference, but the best talks in my view are people that know their stuff, have something to say and just deliver it as their genuine selves.","tags":"Software Engineering","url":"https://avengerpenguin.com/commercial-conferences/","loc":"https://avengerpenguin.com/commercial-conferences/"},{"title":"Connecting Engineering Work to the Business","text":"Mostly notes from Connecting Devs' Work to the Business Bottom Line - YouTube . The main thing that helps is focusing on metrics. We will always have more problems than people. Avoid Shipping Theatre -- in the absence of clarity (clear metrics, clear impact), we simply celebrate shipping things. You chase the deliverable, not the proof that it helps. The most important things might be the hardest things. They might need the most collaboration. Those that avoid this will \"ship their org chart\" and deliver what they can easily and naively deliver without too much collaboration. Metrics used might vary based on what we are doing: Launch a new product? Metrics to show growth, adoption. Working on long-standing product? Metrics more focused on how you'd like to grow/improve it. Weekly review of: Innovation pipeline Execution review Operations and maintenance metrics Metrics are the start of the story. Generally, they tend to point you at some human element behind the metrics or something in how humans, teams and systems are interacting Cycle time is a key metric and it shows how quickly code is reaching the customer. Teams that have short cycle times are likely the most efficient. Speed and resiliency are features in the current era of SaaS and highly connected systems. Production incidents are an amazing learning tool. Blameless incident write-ups that focus on learning are useful. It can find where we might need to be investing time. A key initiative is reducing the feedback loop around the gap between the problem and the solution. Things ranging from experiments to zooming in on individual customers will help here. Makers' Week -- One week a month without standing meetings and people focus on building things for their customers.","tags":"DevOps","url":"https://avengerpenguin.com/connecting-engineering-work-to-the-business/","loc":"https://avengerpenguin.com/connecting-engineering-work-to-the-business/"},{"title":"DevOps","text":"I use a holistic, broad definition of DevOps as a mix of technologies, practices and culture. See What is DevOps? for a full definition. Other topics in this space: Undifferentiated Heavy Lifting Premature scaling can stunt system iteration Technical Debt","tags":"DevOps","url":"https://avengerpenguin.com/devops/","loc":"https://avengerpenguin.com/devops/"},{"title":"Digital Garden","text":"The concept of a Digital Garden is explained well by both Joel Hooks and Christian Tietze . It is mainly a return to the idea of writing notes/posts that are evergreen, non-chronological (i.e. not a blog) and contain lots of interlinking and curation. This is easily illustrated by looking at the idea of a Wiki (e.g. Wikipedia) where you have a single note/page per concept and you favour linking off to related concepts. What makes this approach a garden (over, say, a stream or timeline) is that it's ok to go back and update, expand on and grow the pages over time. I have personally felt a lot of pressure with writing a complete blog post to make it well-structured, coherent and complete before publishing. This leads to perfectionism and Analysis Paralysis that prevents me writing at all. I have always preferred this approach to my own blog, preferring punchy links such as /what-is-devops/ for my post on what I think DevOps is . I've been doing this since reading Kristian Glass's 2015 article on preferring permlinks to having dates in URLs and have configured static site generators like Pelican for some time to prefer this. However, even in my own blog post above, I still have a date in the article and the default behaviour of many static site generators is for the homepage to be a stream of content rather than an old school home page . With the Digital Gardening approach, I plan to write more in the open and build daily habits to write more. Tools: Obsidian Voltaire","tags":"misc","url":"https://avengerpenguin.com/digital-garden/","loc":"https://avengerpenguin.com/digital-garden/"},{"title":"Domestication of Users","text":"Notes from article by [Rohan Kumar](https://seirdy.one/) . The concept of \"User Domestication\" captures the idea not only do we know full well that many messaging or social networks: do not interoperate rely on that lack of compatibility to create a network effect where people need to join to speak to people they know but also that this leads to an inevitible \"domestication\" where users are unable to resist or fight back when that company starts to do user-hostile things like violate privacy or force invasive advertising.","tags":"misc","url":"https://avengerpenguin.com/domestication-of-users/","loc":"https://avengerpenguin.com/domestication-of-users/"},{"title":"ENTP Productivity","text":"I have a few descriptors that have been useful over the years to describe my neurotype and approaches to life. Whether we call it ENTP , Hyperlinker or Plant , I have worked a lot on how best to structure my days and work to go with the grain of the underlying neurotype at play here (rather than constantly fight it). Problems we encounter The main problem is being good at understanding things quickly and starting things -- including finding new ways to come at things -- seems to be anathema to being able to finish and follow through. Possible causes In How to be Organized as ENTP it is hypothesised the issue is a natural outcome of that strength to see new ideas and possibilites. That is, it is hard to stop seeing them even when trying to following through on an existing plan. This attention problem arguably puts these traits on the spectrum for ADHD. Whether ADHD is actually diagnosable or not, the methods for coping with it have a strange habit of being applicable for those identifying with decriptors like ENTP or Hyperlinker. The article above goes on to present a tension between being good at coming up with systems, but then a spiral emerges where intution keeps trying to pull you away from a prescribed plan. Solutions to try Again, from How to be Organized as ENTP , the solutions suggested are generally to keep things minimalist and simple. Daily Tasks Use Bullet Journalling. Or at least some way to distill down a summary 2-10 things that need done today and defer the rest. I have gone back and forth on that approach and the biggest benefit I found was the chore of having to re-write a carried-over tasks would eventually frustrate me to the point of doing the task right now or perhaps just dropping it as it's clearly not valuable to me. People There can be a lot of risk of being too busy to keep in touch with people. Also, we can end up with large lists of acquaintances without keeping in touch with actual friends. The minimalist consideration here is to focus on a small list of people that make you happy. Projects I have found that projects are hard to manage in a Bullet Journal system. The \"Getting Things Done\" approach just has a big list of projects and -- at least for me -- this can be quite a large list. One solution here is to manage it more like a software product backlog, notably: Be clear on what is not being worked on right now A clear project will have an end in mind A minimalist approach is to define an MVP for each project Therefore when you add tasks to a project, you can drop or at least park activities that don't directly feed into that MVP Things I have found I can have a clearer focus in more minimalist surroundings like hotel rooms. I have read The Life-Changing Magic of Tidying which inspired me to declutter quite a lot of things. Browsers tabs and bookmarks I keep a lot of browser tabs open for things I plan to \"come back to\". One thing that helped me here is the Todoist browser extension where I can quick-add any web page to my Inbox or any project and then I can close the tab. Another technique is writing this right now. Any links I want to keep for \"reference\" are now instead transferred into my Digital Garden as either a short note or a longer article as I grow my own thoughts around it and connect it to related articles. Writing this allowed me to close the tab for How To be Organized as ENTP at the very least. References Articles that have fed into this: How To be Organized as ENTP","tags":"Productivity","url":"https://avengerpenguin.com/entp-productivity/","loc":"https://avengerpenguin.com/entp-productivity/"},{"title":"Effective Remote Working","text":"Retros Remote retros can be more effective if we maximise the value from the \"synchronous\" time by doing various things ahead of the call. For example, people can raise all their issues on board, comment on other issues, vote on ones they care about all before an actual video call. Then the video call is focused on the actual discussion portion. 1-to-1 Meetings Arguably the 1-to-1 meeting is better remote in person. They are automatically private and it's less awkward than having to sit face-to-face.","tags":"Productivity","url":"https://avengerpenguin.com/effective-remote-working/","loc":"https://avengerpenguin.com/effective-remote-working/"},{"title":"Empyrrhic Victory","text":"When you destroy your opponents with facts and logic and then have no friends left.","tags":"Rationalism","url":"https://avengerpenguin.com/empyrrhic-victory/","loc":"https://avengerpenguin.com/empyrrhic-victory/"},{"title":"Essentialism","text":"Concept discussed in Essentialism: The Disciplined Pursuit of Less by Greg McKeown . Not to be confused with Essentialism .","tags":"misc","url":"https://avengerpenguin.com/essentialism/","loc":"https://avengerpenguin.com/essentialism/"},{"title":"Every line of code is a liability","text":"It's likely not stated like this verbatim, but one insight I got from Eric S. Raymond's Cathedral & the Bazaar (one of the later essays) is that it's at best naive and at worst incorrect to treat code as an asset. The way we have defined code as Intellectual Property like any other intangible work (writing, music) leads us down a path of therefore treating all code written within a company as an asset of that company, with the developers themselves as costs due to their salaries. Developers are not just costs Firstly, Eric S. Raymond does explicitly point to cases where in fact the developers can be seen as assets. If we see software in terms of present and future value for its users, then developers who can support, fix and otherwise leverage their domain expertise are assets of the company. Sure, their salaries are costs. Code is not always an asset Secondly, we can deconstruct the idea that code itself is an asset by starting with the extreme consequence of that assumption and working backwards. If we take this assumption at face value, we step into the old cliche in software engineering of why we don't pay developers per line of code produced. That is, if code is intrinsically as asset by virtue of being a form of writing alone, then we can immediately falsify that notion with a thought experiment of a team of developers writing lines and lines of pointless code. Maybe just good code is an asset? Ok, so it's not that all code is automatically an asset, but maybe we can still consider \"good\" code or \"valuable\" code to be an asset? However, we all know full well that nobody is scrambling to get hands on code produced by vendors that no longer exist nor has there been any reason for us to see source code releases for old versions of Windows or older games. We could try to argue that this is due to asset depreciation, but I prefer a different analogy to financial accounting. Code as a liability My preferred analogy to accounting ledgers is to consider code itself as a liability, not an asset. That is, each line of code -- or perhaps some more appropriate unit -- is a cost to a business in terms of having to write it, test it, maintain it. Each line increases the surface area for bugs to appear or for developers to break behaviour unintentionally. The reason we write code at all then is because there is an asset that arises from it, which is the value for its users now and in the future. I will buy a game because of the future hours of enjoyment I get out of it and the price I will pay will be based on that enjoyment, not necessarily the amount of code to produce it. To put it another way, if a company like Nintendo could sell me the same game at the same price, but with half the code to produce, test and patch post-release, surely it's not controversial to say they would take that option? This model still allows for the idea that indeed it may take more code to produce a highly immersive, e.g. open world game with a huge amount of playing area and impressive graphics, music etc. This is fine as the company chooses to take on more liability for a better return -- perfectly normal business practice. In fact, this boosts the validity of this model as we can clearly see that a company that invests heavily in such a complex game that then doesn't make it to market has lost out a huge amount more than one that was producing a smaller game. Granted, a lot of that is time, but note how there's no way to \"sell off\" the unused code \"assets\" like they could with other real assets. Maximising value So now the game is how to I maximise that value for customers with the least amount of code needed to produce that. This plays nicely into the ideas that The best software has the fewest moving parts such that not only is less code a smaller liability but we can more readily make changes -- and therefore produce more value -- on top of minimal code. So why all the bloat? A brief consideration of the model above might raise the immediate question: if this model holds, then why is it so common to have lots of spaghetti code and software bloat? Wouldn't businesses be striving for minimal code everywhere? The simple explanation for this is that code is not the only cost or liability at play. There's time and there's also some liability in knowledge gaps among less experienced (and let's be honest also experienced) developers. That is, it might take longer to get a more minimal code base. It might require expertise your developers do not have. It might be we could achieve less with a library or framework, but that brings on other liabilities like security (see the framework dilemma below). Also, to consider again the accounting analogy, we might think of a company taking on debt as a liability, but only minimally to allow them to invest in assets and profit that should exceed that debt, However, we know full well not all decisions are fully calculated. A company may invest in the wrong thing or have to borrow to pay out fines for previous mistakes made. The model that all code is a liability crucially holds in a world where we accept we will accidentally take on more liability than would be entirely optimal. That means excessive liability due to spaghetti code, poor design or bad decisions is our job in software engineering to manage. The Framework Dilemma Finally, there is one interesting dilemma or debate that can fall out of this \"code as a liability model\" particularly when we consider the idea that The best software has the fewest moving parts . Take for example in Java where I could have: List list = ...; for (int i = 0; i < list.size(); i++) { System.out.println(list.get(i)); } This C-style for loop has multiple statements and moving parts where we could introduce bugs. We therefore may prefer in more modern Java to do: List list = ...; list.stream().forEach(System.out::println); Here we get into the power of declarative code being easier to observe as correct. However, have we really reduced the moving parts here? At the end of the day we are iterating over a list from 0 to its length, just in the latter case the code is part of the Java standard library. What's really the difference between the complexity happening in my code versus inside the standard library? This becomes a real debate when we consider frameworks and 3rd party libraries. I have seen people argue both ways which is more complex between, say using a dependency injection library: public class MyClass { @Inject MyOtherClass other; } over just constructing objects yourself: public class MyClass { MyOtherClass other; public MyClass(MyOtherClass other) { this.other = other; } } Clearly the version with Dependency Injection is \"less\" and in fact has no imperative code, but to achieve that we pull in a whole load of 3rd party libraries. So perhaps it's not so clear cut which final system has the least complexity and liability? For the purposes of our liability model, we can leave this as an open debate, but I personally favour the library/framework delegation due to the idea that The Open Source Hive Mind is smarter than you . That is, complexity pushed into a library/framework that has multiple users finding bugs is favourable to complexity isolated in your code based with perhaps orders of magnitude less battle testing. This argument gets harder to justify if you're bringing into a huge framework just to do one small thing. Or when people in the npm ecosystem hand off to packages very trivial things like isarray . As with most things we do in software, the rules aren't absolute, but we should continue to debate as it's in those discussions that we learn and improve.","tags":"misc","url":"https://avengerpenguin.com/every-line-of-code-is-a-liability/","loc":"https://avengerpenguin.com/every-line-of-code-is-a-liability/"},{"title":"Explore-Exploit Dilemma","text":"The Explore-Exploit dilemma is the ongoing struggle to balance bringing novelty, new ideas and experiences into your life with slowing down and getting value out of where you currently are. I got the phrase from Thomas Frank who may have coined it or got it from someone else. This is definitely a big feature of ENTP Productivity .","tags":"Productivity","url":"https://avengerpenguin.com/explore-exploit-dilemma/","loc":"https://avengerpenguin.com/explore-exploit-dilemma/"},{"title":"Fewer remits per team, fewer teams per remit","text":"I coined this for myself as a laudable goal for an engineering organisation after observing the converse for too long. With many years working at the BBC, but also looking at other companies, I saw a pervasive problem that -- with the wrong culture -- we can reach a point where it feels nearly impossible for anything to get done. Anything that does get done takes months or even years and it's not always even clear why in restrospect. The hypothesis I formed here is that we have two factors that cause each other: People end up being \"across\" too many concerns or projects Projects or remits therefore end up with a lot of people covering them What this looks like in practice is meetings with 16 people to \"align\" or agree an approach before a project starts. It looks like people then not following up on actions from regular meetings like this because they're giving their attention to multiple things. It looks like people bike-shedding and giving superficial feedback on things because they don't know enough to discuss it in detail or understand the context. In short, it looks like an organisation where every initiative has a minimum amount of time to deliver due to having to get all the right people gathered to discuss. What's the alternative? Autonomy and clear remits for individual teams. When a team owns a problem space fully (within certain constraints clearly given) and other teams stop having a say in how they solve that space, they can move more quickly. They can experiment with things, try new approaches and feel better job satisfaction from being able to get stuff done. This is a key aspect of my Technical Vision for any organisation. With appropriate autonomy, teams get quicker feedback on ideas and try more things.","tags":"DevOps","url":"https://avengerpenguin.com/fewer-remits-per-team-fewer-teams-per-remit/","loc":"https://avengerpenguin.com/fewer-remits-per-team-fewer-teams-per-remit/"},{"title":"Flow","text":"Flow Factors Focus Flow requires total focus, so removing distractions is key. Might be good to start with a focus exercise. Freedom Includes freedom to make mistakes and not self-edit. Feedback Flow requires a constant feedback loop. Games do this well. This overlaps with theory of deliberate practice. Four % Challenge should be 4% greater than your skills.","tags":"Productivity","url":"https://avengerpenguin.com/flow/","loc":"https://avengerpenguin.com/flow/"},{"title":"Foundation of mathematics in Python","text":"This is an exploration of building up Zermelo‚ÄìFraenkel set theory as a foundational basis of mathematics, but actually implemented in Python. There is no clear application for any of this other than the challenge of trying to reimplement arithmetic (inefficiently) without being able to use literal numbers. The challenge here is to recreate arithemtic operations such as addition and multiplication and representations of sets of different kinds of numbers using only a \"set\" object in Python. No literal numbers are allowed and we may even try to avoid lambda so that we are forced to recreate functions. What we will need to do though is allow the use of class to wrap the native Python set and use operator overloading to allow us to define operations of that set as we can justify them from axioms. Part of the reason for this will be the need to represent infinite sets which requires some pairing up of sets and generators. Firstly, we start from a very basic assertion: Things Exist The most philosophical foundation before we get into what we can be sure exists and is correct is to accept that at least something exists (as in \"I think therefore I am\"). If we imagine that it is reasonable to group things together arbitrarily, then it follows we can create some empty grouping we can call the empty set: ```python trusted=true √ò = frozenset() √ò, len(√ò), 2 <!-- #region --> Note for now we can just use Python's built-in concept of `frozenset` since it has all the properties we want for now (i.e. it's empty). ## Axiom of Infinity <!-- #endregion --> ```python trusted=true class Set(frozenset): def __init__(self, definition=frozenset()): if hasattr(definition, '__call__'): self.gen_func = definition else: self.gen_func = None #super(definition) Set() ```python trusted=true # Empty set „Äá = frozenset() # pair ùï° = lambda a, b: frozenset({a, b}) ùë∫ = lambda domain, cond: (x for x in domain if cond(x)) ```python trusted=true ùï° = lambda a, b: frozenset({a, b}) ùëÉ = lambda a, b: ùï°(ùï°(\"l\", a), ùï°(\"r\", b)) class Set: def __init__(self, genf=set): self.genf = genf self.vals = set() def __iter__(self): gen = self.genf() for n in gen: if n not in self.vals: self.vals.add(n) yield n def __or__(self, o): def _gen(): A, B = iter(self), iter(o) while A and B: if A: try: a = next(A) yield a except StopIteration: A = None if B: try: b = next(B) yield b except StopIteration: B = None return Set(_gen) def __mul__(self, o): def _gen(): out = set() A, B = iter(self), iter(o) while A and B: try: a, b = next(A), next(B) except StopIteration: break c = ùëÉ(a, b) if c not in out: out.add(c) yield c return Set(_gen) „Äá = Set() succ = lambda n: Set(lambda: {n}) | n class Naturals: n = None def __next__(self): self.n = „Äá if self.n is None else succ(self.n) return self.n def __iter__(self): return self ‚Ñï = Set(Naturals) add = lambda ùëö, ùëõ: Set(lambda: (ùëö * {„Äá}) | (ùëõ * {succ(„Äá)})) set(add(„Äá, „Äá)) ```python trusted=true ùëÉ = lambda a, b: ùï°(ùï°(\"l\", a), ùï°(\"r\", b)) ùëÉ(\"a\", \"b\") ```python trusted=true left = lambda p: {r for r in {q for q in p if \"l\" in q}.pop() if r != \"l\"}.pop() right = lambda p: {r for r in {q for q in p if \"r\" in q}.pop() if r != 'r'}.pop() left(ùëÉ(\"a\", \"b\")), right(ùëÉ(\"a\", \"b\")) ```python trusted=true ùëì = lambda P: lambda a: next(right(p) for p in P if left(p) == a) succ = ùëì(ùëÉ(n, frozenset({n}) | n) for n in ‚Ñï()) len(succ(succ(„Äá))) ```python trusted=true class InfSet: def __init__(self, genf): self.genf = genf self.vals = set() def __iter__(self): gen = self.genf() for n in gen: if n not in self.vals: self.vals.add(n) yield n def __or__(self, o): iterators = (iter(self), iter(o)) while iterators: for v in map(next, iterators): yield v ‚Ñï2 = InfSet(‚Ñï) S = ‚Ñï2 | ‚Ñï2 for n in S: print(len(n))","tags":"notebooks","url":"https://avengerpenguin.com/foundation-of-mathematics-in-python/","loc":"https://avengerpenguin.com/foundation-of-mathematics-in-python/"},{"title":"Four Burners Theory","text":"The origins of this idea seem to be murky, but most people point to the James Clear article on the Downside of Work-Life Balance . The idea is that you visualise health, work, family and friends as four seperate burners on a gas hob. The belief is that you need to turn one off to be successful or perhaps even turn off two in order to be really successful. This is presented as a hard constraint -- there's no easy workaround (e.g. a standing desk alone isn't enough to maintain health while working). It's part of a mentality that life is all about Trade-offs. Solutions The options of how to deal with boil down to: Keep all four going and don't excel in any of them Outsource or delegate (or automate?) Use constraints, prioritise and otherwise make better use of your time (automation can help here too) Embrace the idea of Seasons of Life 1. Maintain all four Both James Clear and The Natural Edge suggest you can burn all four at a mediocre heat, but I wonder if this is one of these areas where if you don't choose the constraint, the constraint will choose you. That is, if work draws all your focus, things like family and health start to get less attention, whether you intended this to happen or not . If this is the case, then we absolutely should be intentional in which we choose to focus as letting one or two burners be compromised unintentionally is likely to incredibly negative. Does this mean this option isn't really an option? Or at least not a healthy one. 2. Outsource James points out we outsource lots of things already: Dry cleaning Cooking (when ordering takeaway) Repairs Childcare Essentially the idea is that outsourcing keeps the burner running, but with you removed from the equation. This might not be fulfilling as just because your children are looked after, doesn't mean that you are spending time with them. Also, if you manage to delegate all your work, you might not want to have days with nothing motivating to work on. 3. Prioritise and Constrain Multiple articles talk about prioritising. An oft-cited approach is Greg McKeown's Essentialism where you should cut out literally everything that isn't absolutely essential to you. This is obviously something we should keep an eye on all the time, but what stands out from James Clear's article is explicitly leveraging constraints to force you to be more efficient. 4. Seasons of Life The concept here is to break your life into seasons. Rather than striving for a perfect balance of everything forever, you have periods where your focus is more on some things than others. In Nathan Barry's article on seasons , they seem to be anything from a few weeks to a few years where you have a particular focus, but then that season comes to an end. This appears to fit nicely with the idea of \"less is more\" as well as avoiding trade-off anxiety where it's hard to pick what to commit to, so you don't commit to anything. If we know that \"I'm doing this for the next 3 months\" then we know things being traded off simply have to wait -- you're not dropping them altogether.","tags":"Productivity","url":"https://avengerpenguin.com/four-burners-theory/","loc":"https://avengerpenguin.com/four-burners-theory/"},{"title":"H Factor","text":"See H Factor .","tags":"REST","url":"https://avengerpenguin.com/h-factor/","loc":"https://avengerpenguin.com/h-factor/"},{"title":"Hacking Remote Work","text":"Synchrony Avoid the need to talk back and forth all day to keep work done. Instead: Regular, standing meeting with a shared document Office hours Set up processes for certain work Control Meeting Availability Be proactive and helpful in allowing people to book meetings, but at times you're ok with. The aim here is to control your meetings without being annoying or difficult. Keep the available windows \"random\" enough as being consistently unavailable, say, after 2pm looks suspicious. Sell Processes Focus on the key area of getting buy-in for the processes you're setting up. If people don't buy in, they will choose to message you. People may think of all the things that can cause a process to break down, so it's worth having an \"escape hatch\" of what to do in the worst case. This will appease them and it's highly unlikely you will use it. Trade Accountability for Accessibility Explicitly ask for some drop in accessibility to go off and work on something big so long as you're then accountable for that. Monthly Crush Make sure you do enough deep work on something valuable such that at least once a month you \"crush\" a project and have something to show. You will gain a lot more autonomy this way.","tags":"Productivity","url":"https://avengerpenguin.com/hacking-remote-work/","loc":"https://avengerpenguin.com/hacking-remote-work/"},{"title":"Impermanence","text":"Today's entry from The Daily Stoic, entitled \"The Smoke and Dust of Myth\", discusses how quickly things -- particularly trivial and petty things -- are quickly forgotten.. Smoke and Dust This is inspired by writings in Meditations from Marcus Aurelius in which he notes how most emperors before him are mainly forgotten: Run down the list of those who felt intense anger at something: the most famous, the most unfortunate, the most hated, the most whatever: Where is all that now? Smoke, dust, legend...or not even a legend. Think of all the examples. And how trivial the things we want so passionately are. -- Marcus Aurelius, Meditations, 12.27 This is a common Stoic theme: at the end, all your anger at trivial things and all your desire and passion to chase fame and possessions will have been for nothing. The Impermanence of Things This transient, impermanent nature of things has played on my mind a lot over the last three to six months. It resonates strongly with me personally as it is a theme explored in one of my favourite Romantic poems: I met a traveller from an antique land Who said: Two vast and trunkless legs of stone Stand in the desert... near them, on the sand, Half sunk, a shattered visage lies, whose frown, And wrinkled lip, and sneer of cold command, Tell that its sculptor well those passions read Which yet survive, stamped on these lifeless things, The hand that mocked them and the heart that fed; And on the pedestal these words appear: 'My name is Ozymandias, king of kings; Look on my works, ye Mighty, and despair!' Nothing beside remains. Round the decay Of that colossal wreck, boundless and bare The lone and level sands stretch far away. -- \"Ozymandias', Percy Bysshe Shelley Here, Shelley beautifully describes a strong contrast between the sneer and the arrogance of an ancient king with the inevitable decay of his impermanent place in the world. His status, achievements and \"works\" may have felt significant -- and worthy of pride -- in this time, but ultimately, was it worth it? This is not just applicable to those seeking to be emperor, however. We can apply this concept in daily interactions at work or in personal relationships. Essentially, if we are disputing something or worked up over a disagreement, we can remind ourselves daily that some of this anger or frustration will be forgotten in weeks, months and years to come. We can ask ourselves of any issue: will this matter in a year? In ten years? Now, there are some things that will matter longer term and there is an irony in Marcus Aurelius's words which I will explore with a personal example. While statues may survive in deserts for millenia, working in software engineering is the polar opposite. The same could be said for a lot of professional or project-based work today, but my experience is primarily in building sofware-based systems and websites. It is incredibly common for long-running, service-based applications to run only for handful of years before being replaced or decommissioned. There are many systems that have been running for far longer than they should, but clearly this is an industry that operates in short time frames if we are shocked to discover things that have been running for over ten years. So, while Eiffel or Brunel have a legacy of physical structures to their name, I am only half way through my own life and witness to whole teams of engineers working tirelessly to replace and tear down whole service and applications I had a personal hand in designing, building and deploying. I have observed many software developers fall into an emotional trap of having a personal investment in code they have written and applications they have worked on. They seek to continue to fix unresolved issues, even when the business does not need them fixed. They want to continue to maintain these systems, even when there are diminishing returns in doing so. At worst, they might get upset or angry at the idea that these applications are no longer valued and are inevitibly going to be decommissioned. However, when we apply what Marcus Aurelius and Percy Shelley both remind us, we accept that if things we have worked on are inevitibly going to disappear, then it does not matter if that is next week, next year or new decade. The software is already transient the moment you worked on it. As software development is usually collaborative within a team, this impermanence puts into perspective any dispute or debate you might have had over the code, design decisions or even how to name parts of it. These debates seem important at the time as you are expected to deliver high-quality results and it is important to \"get it right\" as much as possible, but maybe we can evaluate our tendency to over-polish something if we are reminded of its temporary nature? People who work in other professions may also recognise aspects of their work that have a inevitible, finite lifespan. You can work incredibly hard on a report, on a software application or any other short-lived thing and its value will disappear. So, does that make our work pointless? Should we abandon it? Not necessarily. This is where we need to dig one level deeper into Marcus Aurelius's and Shelley's words above. Legacy Marcus Aurelius was right to write meditations over the forgotten emperors before him and he was right to keep humble over his own anger and passion in light of that. He was right because he was unable to foresee the irony his words would acquire: he himself was not forgotten. We are reading his words some two millenia later and gaining wisdom and value from them. In a sentence, we have not forgotten his words reminding us not to fall foul of anger or passion because these things will be forgotten. This applies again to our working and personal lives. When we work on projects like software applications, papers or other things with transient outputs, we are learning . We might use the project to try a new approach, we might have to solve a problem we have not had to solve before or we might use our work to train less experienced people. I have worked on at least two major projects from which I have been able to extract papers and talks. As I focused on using project work to learn , improve and then share these things with others, I built more permanent value out of them. Specifically, it is when we focus on the Stoic principle of virtue in sharing our learnings that our wisdom and ideas have potential to live beyond us. We won't all be remembered as Charles Darwin or Isaac Newton for contributions to the body of human knowledge, but it is not likely that these people did their work with the intent of being remembered for centuries either. They worked and shared that work with others. Your software or other project likely provided some value to someone for its lifespan. That was the value and virtue it provided and, I would argue, that is the primary output of any project -- not the superficial output many assume to be the main deliverable. That value may have had lasting effects on those who consumed that output, but even if it didn't you owe it to yourself to create more value by focusing on the aspects of work that improve you and improve others around you. You were paid to create something, but you can choose to extract more than just pay from the work. I am happy to witness or even have a hand in tearing down things I worked hard on. As I strive to use each project to learn, grow and develop myself then anything created by a past version of myself is, by definition, built by someone less competent than me.","tags":"Stoicism","url":"https://avengerpenguin.com/impermanence/","loc":"https://avengerpenguin.com/impermanence/"},{"title":"Infrastructure Sandwich","text":"There is a difficulty that arises, I have noted, when a distinct tool or pipeline step is used to deploy application code compared to deploying infrastructure. If we are doing infra-structure-as-code or GitOps it is likely we have our infrastructure in the same repository as the application that runs on it. It is also likely we deploy that infrastructure via Terraform or a vendor-specific tool like AWS Cloudformation. However, teams may choose to do application deployment on that infrastructure via kubectl in the case of Kubernetes. Maybe we have a serverless application that we deploy via a framework's own CLI tool. The Sandwich So we likely need a deployment pipeline that looks roughly like: However if we take the Kubernetes use case, we the \"App\" deployment will include things like ingress. If we have used the ALB Controller to create an AWS load balancer, we can't deploy any infra attached to that until after it is created. Therefore maybe we want: But this is clearly wrong if we want to provision infrastructure the app needs like a Redis cache or database before the application is deployed. So the \"Infrastructure Sandwich\" emerges as a naive solution to the problem: That is, we have two deployments for infrastructure. Firstly we deploy the base infrastructure needed for the application, then the application and finally we can deploy additional infra like alarms or logging that rely on knowing things created by the application stage. The Problem The most obvious problem is that the pipeline above just looks clunky. It doesn't seem elegant to have this sandwich of deployment steps and that inelegance is telling us something about our design and abstractions, I think. A more tangible issue is we have lost potential atomicity of our deployments. That is, if we deploy everything through a single AWS Cloudformation update (a feature of AWS SAM) then Cloudformation handles: Making the change appear as a single operation Doing internal steps in an appropriate order to avoid bad state Halting and rolling back the transaction if an unexpected error occurs Tools like Terraform do not tend to roll back as such, but there is an atomicity of the \"plan\" phase where we have old state, desired new state and it plans all the steps to get from one to another in a single operation from the perspective of the caller. But people don't always notice it There's a particular tricky aspect to this problem that makes it hard to notice it. In fact, reading this you may be thinking this isn't a big problem or an edge case. One of the things that hides this problem is if we build a pipeline like the first one above: as a first pass. That is perfectly sufficient to bootstrap base infra (EC2, Kubernetes) and deploy to it. Then later we can add databases, caches, etc. and it will work. And then one day we add an alarm attached to a load balancer, say, and it will work . The truth is if applying deltas to existing infrastructure, we can easily attach stuff to existing load balancers and other things owned by the \"app\" deployment phase because those things in place. But can you redeploy all this from scratch in case of disaster recovery? What will happen if all the infra is lost and we run terraform plan for the first time, it will abort because it's trying to attach infrastructure to things that do not exist yet. This is such a hard thing to notice and get right but I have seen countless cases where infra-as-code and continuous deployment is reliant on the fact we haven't ever actually torn it all down and verified it will all come back up without some chicken-and-egg paradox. Causes There's a few places I have seen this arise: Where Kubernetes is used on a cloud vendor and you want to let it create some infra like ingress (and AWS ALB vis the ALB controller) but you want to attach alarms to that ALB. Where an internal platform tool was used to create Amazon Machine Images (AMI) and upload them to an existing AWS Autoscaling Group (ASG). The presumption was you had handled an infra deployment phase beforehand. Any case where deployment is a little bit more manual, e.g. I create infra, deploy via git, FTP, SCP, then add some more infra. In these cases it's normally so manual people don't even have a pipeline in which the sandwich appears. Cases where a serverless framework (e.g. serverless) is used where it has built-in deployment commands that are limited to creating the function and uploading the latest code. In these cases we still need infra deployments either side. Solution While the two-phase infra deployment will do it, it's hinting we have our abstraction wrong. In my mind this is where encapsulation comes in. The naive pipelines above are based on a domain model along the lines of:","tags":"DevOps","url":"https://avengerpenguin.com/infrastructure-sandwich/","loc":"https://avengerpenguin.com/infrastructure-sandwich/"},{"title":"It's either HATEOAS or it's RPC","text":"Life is simple when you are working with only one computer. Distributed systems are hard. In fact, if you build software for a living or a hobby, can you think of a time you last wrote something that didn't involve some kind of inter-machine communication? Even with the shift back to bigger client apps on smartphones, many require communication back to a central service via an API. It's arguably safe to assert even without real evidence that the most common -- if not one of the most common -- approaches taken in client-server architectures is for the server-side to present a so-called RESTful API for the client application to consume. The REST Architecture Many software developers, particularly those building web applications, will have a passing familiarity with the REST architectural style in simple terms: Different URLs are used for different \"resources\". You use all the HTTP verbs correctly: GET is read-only and cacheable, DELETE is used to remove a resource, etc. All requests are kept stateless. And we're done. Oh, what's that? I forgot to talk about HATEOAS? For those not familiar, HATEOAS is one part of the \"Uniform Interface\" constraint. Let's look at those constraints in full for context: Identification of resources -- This basically amounts to using URLs for each resource as described above (assuming we're using HTTP as the protocol, of course...) Manipulation of resources through representations -- Send a resource represented in e.g. an XML or JSON format of some kind and that should be enough for a client to read or update the resource. Self-descriptive messages -- Use headers like Content-Type to make it clear how to parse responses. Hypermedia as the engine of application state (HATEOAS) -- A client should only be able to move between resources by following hypermedia links and controls on a particular representation. The definition of HATEOAS above is an attempt to summarise it in a single sentence, but if you're not familiar with it at all, it's not likely to be any clearer that that actually means but also why you might use it. In fact, Roy Fielding -- who coined REST in his famous PhD thesis -- has himself in the past attacked HTTP APIs that claim to be \"REST\" APIs when they do not. There have also been examples of disagreements on whether HATEOAS is needed for a REST API. That there is a significant level of confusion or even disagreements and arguments suggests a lack of clarity around what HATEOAS truly is and why you might use it. The argument usually boils down to people pointing that out failure to follow the REST constraint does not a REST API make. The common retort is that it is zealous or purist to ask for this constraint to be followed. A whole camp of API designers will furthermore claim the REST-minus-HATEOAS architectural style to be valid in its own right. Some even use terms such as [Pragmatic REST}(http://www.ben-morris.com/pragmatic-rest-apis-without-hypermedia-and-hateoas/) to describe/justify/differentiate their \"non-purist\" API design. I'd like to take a totally-non-zealot, rational position on this: Of course you can build a working API following all REST constraints minus HATEOAS (many people clearly do it every day!) If you do build a so-called \"pragmatic REST\" API, then calling it REST might invoke ire from purists, but more importantly it's not helpful to call it REST. Whether you should follow REST 100% or not is ultimately up to you, but you're missing out on some of the core benefits of the REST architectural style (and why it exists) Also, if you're not going to benefit from the advantages of the REST architecture, it's not clear why you need to follow some of the other constraints. You might be making a trade-off you don't need. I will attempt to justify these statements one-by-one, with the exception that I hope that 1) is obvious. Why can't I call my API RESTful? Semantic debates are fun, aren't they? I can build an API and call it a REST API, then someone can say it isn't truly REST and now we're back to arguing about what is and isn't REST. What's missing in that hypothetical argument? Something that's missing in a lot of discussion on the web about this: any concrete discussion of the properties of the thing originally built. There's two distinct fallacies that appear when a debate around a definition or semantics emerges: we talk too much about whether something truly fits a definition without actually discussing the merits of that thing ; or we dismiss opinions of those who disagree with us by incorrectly regarding a debate as being \"just semantics\". If you already have an opinion on the REST vs. \"pragmatic\" debate, you might well recognise one of those fallacies in those with whom you disagree, but be careful of falling into the other fallacy in your own views. Both of these fallacies can be seen as extreme ends of a spectrum where at one end we get sucked into pointless debates over definitions and at the other end we see no value in discussing definitions at all , which is also irrational. It is indeed the case that an argument about whether my API is RESTful or not is not going to make much progress if we only discuss back and forth as to what REST is or whether it's useful to use the word only in a purist sense or whether we can allow for \"pragmatic\" takes on the term. What's lacking here is discussing it in the context of the API I am building . Conversely, we should not be so quick to shoot down those that point out where my API does not conform to REST. It is tempting to call such a person zealous or a purist and frame any of their arguments as irrational and therefore not worth hearing by definition. The more rational, balanced position is to discuss the definition in terms of why all the REST constraints exist and also in the context of a particular problem domain being solved by a \"RESTful\" API. Here we can rationally see if the constraints not being followed would help or hinder us; we can revisit the advantages brought by lesser-used constraints such as HATEOAS. In this context, the definition is certainly useful and we are not longer arguing \"just semantics\". So, surely it's still my business whether I call my API RESTful or not? Well, sort of. Semantics -- as in the meaning of words -- relies at least to some extent to people loosely agreeing what words mean. If you present a software system or a technology and use certain words to describe it, then expect confusion when you use a word differently to how others use it. I'm not saying we need to be overly pedantic about words, but if you build an API and call it RESTful, expect some confusion by some who will expect to call it in line with all of the REST constraints. In some ways, I would plead with people to avoid using the term REST if they are consciously choose not to follow the architectural style fully, but I fear we may have lost control of the term by now to the point where it's hard to trust any meaning has been followed. In practical terms, it means that if someone presents an API to me and tells me it's a REST API, I still don't quite know what to expect until I see it. It is less than ideal for any word to get to a point where it doesn't actually convey information any more. This is especially true in a technical industry like software engineering. Advantages of HATEOAS Why was REST even created? What is HATEOAS? Why is HATEOAS a good thing? Other RPC Approaches Java RMI CORBA Thrift and Protocol Buffers SOAP JSON-RPC Where these are better than REST Are any of these appropriate for me?","tags":"REST","url":"https://avengerpenguin.com/its-either-hateoas-or-its-rpc/","loc":"https://avengerpenguin.com/its-either-hateoas-or-its-rpc/"},{"title":"The Simplest Way to \"Keep Up\" With the JavaScript Ecosystem","text":"I often hear the frustration or jokes around how fast the JavaScript ecosystem moves. Observations usually include things like \"there's a new framework every week\" or that you need a huge number of tools just to get started on a basic modern application. The frustration can come from developers who either feel they need to learn several tools just to get going, whereas in the past you just needed a text editor and knowledge of HTML, CSS and JavaScript itself. It can be even more frustrating when you have large application UIs built, for example, in Angular but now everyone is using React . Your build process is fully dependent on Bower and browserify but now everyone is using Yarn and Webpack . This is particularly difficult for enterprise application developers as you tend to end up with large code bases where you have little justification going in to \"modernise\" or refactor things if the application works just fine. This is where Java was traditionally favoured in the enterprise as the language (and platform) changes slowly, is (almost) always backwards compatible (so you can upgrade tools without changing code) and the toolchain hasn't moved that quickly either. Yes, we have Ant , Maven and Gradle now, but that's been a slow and steady progression. So, clearly we don't want to move at a break-neck speed and keep refactoring working code bases, but we also want to be able to held back by slow-moving platforms. People might like the fast innovation and choice in the JavaScript and Node.js ecosystems, but how do we keep up with all the frameworks and tools? Well, I have one suggestion to cope with \"keeping up\" with all the new tools and libraries and frameworks coming out each week: Don't. Seriously, don't keep adopting new technologies as they emerge. While a new build tool or framework might seem nicer than what you have, wait until you have an actual problem the new tool solves and be sceptical about migrating your applications to new frameworks. Even for newer projects, carefully consider the cost of adopting a new tool in terms of having to learn it. Maybe stick with what you know and play with new tools only on side projects or in \"10% time\" outside of your main work. For frameworks whose popularity is short-lived, you will be able to skip right over them and only move to things that stand the test of time. For frameworks that stick around, you will be able to adopt them when best practices are established and early bugs are fixed. Learn to live with applications in older frameworks and on older versions of Node.js and you will be able to focus more time on adding features and perhaps even working on new, exciting things.","tags":"Software Engineering","url":"https://avengerpenguin.com/keeping-up-with-javascript/","loc":"https://avengerpenguin.com/keeping-up-with-javascript/"},{"title":"Keras","text":"Keras is an API frontend for machine learning written in Python capable of running on to of TensorFlow , JAX or PyTorch. Here is an example of training a Sequential model with TensorFlow: import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from sklearn.model_selection import train_test_split import numpy as np # Generate some synthetic data for demonstration np.random.seed(0) X = np.random.rand(1000, 5) # 1000 samples, 5 features y = np.random.randint(2, size=(1000,)) # Binary labels (0 or 1) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Build a simple feedforward neural network model model = Sequential([ Dense(10, input_shape=(5,), activation='relu'), Dense(1, activation='sigmoid') ]) # Compile the model model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy']) # Train the model model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1) # Evaluate the model on the test set test_loss, test_accuracy = model.evaluate(X_test, y_test) print(\"Test Loss:\", test_loss) print(\"Test Accuracy:\", test_accuracy)","tags":"Software Engineering","url":"https://avengerpenguin.com/keras/","loc":"https://avengerpenguin.com/keras/"},{"title":"LeadDev Unwrapped 2022","text":"Summary of various talks from LeadDev Wrapped | LeadDev Free virtual event . Being a Principal Engineer Senior IC is a technical leader, which involves decision making as well as leading Should report to a VP, peered to managers Doesn't manage people directly People who have \"seen things\" and can help avoud known risks Bridging product strategy and technical execution Identifying \"one way\" doors Saying \"it depends\" a lot Some basics: Just because you can doesn't mean you should How can I help? What are the consequences of coupling these systems? How will we interface with X if Y goes down? Is this written down anywhere? Starting new job Questions Why were you hired? How can I build my network? Look at org chart Make a contact in each CTO-reporting function Look for places that violate Conway's Law Find human connections with coupled systems Set up 1-2-1s with ICs reporting to your manager Build team context Build peer network -- people you can share ideas with Meeting and Information access -- how to get information from your leader Comms cadence -- min 30 mins per fortnight What's your job? What (wrong) lessons did you learn? Things from pervious job What can I tackle now? What can you uniquely do as a new person? Tackle paper cuts Learning? Am I in to level up the org? Or to help ship? Do they need to do more or spend more time researching? Technical breadth -- draw an infra diagram, app arch diagram, map out a request Map out steps to deploying code Pair with people Delegation Give opportunities to unrepresented groups -- they need opportunity, not advice Lend your privilege Strategy Vision -> Strategy -> Tactics Strategy: effectivness, tactics: efficiency Influence Create a great personal brand Be known for finishing projects or clearly explaining why they died. Learn to say \"no\". Really listen. Wait 30 seconds after you're ready to jump in. Give credit. Build a network. Give influence to other influencers. Being a better IC after a year in leadership Be honest about needing time to learn and upskil Find \"people\" people on the team to aid you in relationship building Help all ICs be better leaders Code Reviews Readability Can someone with lower context understand what the code is doing? Prod scale Lots of concurrent requests? Does it scale with database rows? User Experience Clear error messages Is all copy clear? Does it work cross platform? Accessibility? Interviewing 0-4 rubric scale for each attribute Leadership and Execution Management vs. Leadership Team management includes social cohesion Delegation Risk \"Things would be better if we just did X\" -- ok, then do it Putting money where your mouth is Being irresponsible has its benefits Make bets with your time Find something you disagree with, take 1 day, chunk it out, prove it Reliability \"Make it work\" Understand the real objective Incremental results Willingness to adapt Set hard deadlines, keep objectives in mind, don't obsess about missing Reputation Don't throw people under the bus Connect people who need connected Think how you present in meetings or writing Relationships Favour gifting -- give someone something they want Reducing Complexity Early execution is \"depth-first\", leadership is \"breadth-first\" Summarisation: Elevator pitches Potential leaders should write more Encourage and reward curiosity to unfamiliar areas Moon before Mars Crossing the A - creating a bridge between teams that otherwise would only have the CTO in common Developer Productivity Focus on value created for the business Get feedback from your team Get Qualitative feedback Eliminate biggest timewasters Invest in leverage Take your team on the journey with you Iterate! People building Position Know our scope Know your level Know your performance Plan What's your motivation? Don't project motivations What's your current path? What's the next step? Progress Designing work Growth areas, Action Items, Things to discuss Reward and recognition","tags":"misc","url":"https://avengerpenguin.com/leaddev-unwrapped-2022/","loc":"https://avengerpenguin.com/leaddev-unwrapped-2022/"},{"title":"LeftValues","text":"","tags":"misc","url":"https://avengerpenguin.com/leftvalues/","loc":"https://avengerpenguin.com/leftvalues/"},{"title":"Less is more","text":"Recently, I was prompted by an entry in my Daily Stoic Journal to consider \"What would less look like?\". The consideration for the day was around the benefits and virtues of being brief and concise with words, but perhaps also with other things in life. This rung particularly true with me as I have made a greater effort to write more often, which I found came naturally when I decided it better to practice writing something daily rather than wait until I have enough free time to write full articles. In fact, as I have trained myself to write less if I am low on time, I have paradoxically found myself writing more words in total this year than in all the years I have been telling myself I should blog more. I have also found better success recently with communicating ideas and teaching new things to people by only talking about small parts of the idea or field at a time. There's likely a reason why university lectures are better kept to an hour at a time rather than trying to spend a whole day talking about one topic. I am sure there are many more areas to apply the \"less is more\" principle and we can be more effective at something by doing less of it. In the spirit of the principle, I won't explore them all today, but I would be really interested to hear other examples in comments or on Facebook/Twitter. Where have you done less of something and then got more effective at it? Does anyone else have experiences to share?","tags":"Stoicism","url":"https://avengerpenguin.com/less-is-more/","loc":"https://avengerpenguin.com/less-is-more/"},{"title":"Lofi Hip Hop Generator","text":"Prior Art https://ninaornina.medium.com/building-ai-to-make-lofi-hip-hop-an-introduction-to-recurrent-neural-networks-fe245be3a87d SaaS Potential Something like https://voxiso.com/","tags":"misc","url":"https://avengerpenguin.com/lofi-hip-hop-generator/","loc":"https://avengerpenguin.com/lofi-hip-hop-generator/"},{"title":"MBTI Alignments","text":"Alignment Character Type(s) Lawful Good Superman ESFJ Neutral Good Spiderman ENFJ, ISFJ, INFJ Chaotic Good V ENFP, ISFP, ESFP, INFP Lawful Neutral Judge Dredd ISTJ Neutral Ent INTP, INTJ Chaotic Neutral Jack Sparrow ENTP Lawful Evil Darth Vader ENTJ, ESTJ Neutral Evil Xenomorph ISTP Chaotic Evil Joker (Dark Knight) ESTP Lawful Neutral Chaotic Good ESFJ ENFJ, ISFJ, INFJ ENFP, ISFP, ESFP, INFP (All FP) Neutral ISTJ INTP, INTJ (Both INT) ENTP Evil (3 ST) ENTJ, ESTJ (ETJ) ISTP ESTP Takeaways Quick: J never chaotic, P never lawful F always good, T never good I/E and S/N blur a lot Worldviews ST -> Evil except for ISTJ being neutral, so fairly evil (pragmatic worldview isn't good) SF, NF -> Good (so all F -> Good) NT -> Neutral except for ENTJ. True neutral when INT, but EJ makes it evil + lawful and EP makes it chaotic. Identities EJ is never chaotic IJ Generally neutral, never chaotic IP never lawful, varies via worldview as with EP EP is always chaotic (worldview drives how good or evil) Core Intelligences NP almost always chaotic except with INTP SP generally chaotic, good with F, evil/neutral with T SJ generally lawful, again neutral/evil with T, good with F NJ never chaotic, mostly neutral, good when with F, neutral/evil with T (so visionary role is quite independent, but F keeps it a good vision) Supplementary Intelligences TP never lawful, never good TJ generally lawful, never good FP entirely chaotic good (strong Fi all round?) FJ always good, mostly neutral (except ESFJ being the nicest one) Dominant Functions Ne brings the chaos Fe is good and lawful or neutral (never chaos) Ti is neutral -- evil when with Se, true neutral with Ne Te is lawful evil Se is chaotic -- evil with Ti, good with Fi Si is neutral good with Fe, lawful neutral with Te (compassion trumps rules or rules trump compassion) Ni is neutral -- true neutral with Te, neutral good with Fe Fi is chaotic good","tags":"misc","url":"https://avengerpenguin.com/mbti-alignments/","loc":"https://avengerpenguin.com/mbti-alignments/"},{"title":"Mentoring","text":"Jargon Jargon is a shortcut to comms. Useful only if there's a prior understanding. How Build trust -- Confidence Assume infinite intelligence, zero knowledge Remove Roadblocks Fear -- safe environment, learning as a value Imposter Syndrome -- Celebrate accomplishments, Reflect on progress, Keep a brag book Formulate Good Responses Encouraging Walk the middle path -- not too much info, but not too little Non-directive Bad Responses Well, actually... It's easy Just XYZ... (directive -- prefer \"when this happens, I recommend...\") Don't know? Hit pause -- Go away and get a valuable response -- say \"I don't know\" Show how to use resources to find things out. Mentor Better Self care -- take breaks, know your limits, be comfortable saying \"no\" Cultural and personal sensitivities -- personal space, acknowledge and appreciate differences Outlive the relationship Act as if you'll be mentor forever Ask for feedback Other situations Code reviews Informal knowledge sharing","tags":"misc","url":"https://avengerpenguin.com/mentoring/","loc":"https://avengerpenguin.com/mentoring/"},{"title":"Mocking Data Only","text":"There are a variety of arguments out there around the most effective approaches to software testing. We have unit tests, functional tests, smoke tests, property tests, integration tests, regression tests, end-to-end tests, user acceptance tests and many more. Many of these terms overlap and clearly a combination of these approaches is required to get a true picture of whether our software is working as intended. We need isolated \"unit tests\" to give rapid feedback as to whether our functions or algorithms are correct and whether edge cases are effectively covered. However, these tests do not prove that the various components assemble together to provide a cohesive, working application. This is where end-to-end or acceptance tests come in. These are usually more holistic, but come at a cost in that the feedback loop can be far slower given the need to compile every component and stage the application somewhere. For example, a mobile app might need the latest iteration installed across multiple mobile devices and then tested by hand. I want to focus today on the area of unit testing, particularly in modern software development where we commonly pull in multiple third party libraries to do as much heavy lifting as possible. I believe this has a lot of impact on how we think about unit testing today and we might need to update our patterns and beliefs. Specifically, I want to talk about the practice of mocking (or stubbing) in unit tests and whether approaches to date even make sense in the current software engineering landscape. First, we must briefly look at how the composition of a modern software application has changed over the last two to three decades. A little (personal) history I personally started out with Java, PHP, Delphi and bits of Python and JavaScript around the turn of the century (something like 1998 to 2002 for this period and I am not counting learning BASIC as a child around 1992). This brought me to these ecosystems later than some engineers older than me, but notably early enough to experience writing simple code projects with no package managers, no build \"tools\" and no Github. For PHP, there was no composer to download packages and I got by with a zip file download of PHP and Apache HTTPD on Windows 98 (later XP) using only the PHP standard library. For help, I had an offline copy of the php.net documentation since dial-up Internet took long enough to get the zip files. For JavaScript, you could easily observe code on other websites to learn the language (minification was rare) and, again, a simple text editor like Notepad was enough and you can test HTML and JavaScript in a browser locally quite easily with file:///c:/My Documents/... For Java, I wrote code in a text editor (even in Notepad) and used javac to compile \" *.java \". It was some years later before I would even use Ant , let alone Maven . In fact, I acquired the JDK for the first time by being handed a pack of four Java CDs at a University open day. Across these CDs were the JDK, other esoteric tools and offlne copies of the Javadocs for the Sun Java standard library. Why am I telling you all this? It's to paint a picture of how downloading libraries from Github or package repositories is as easy as breathing today, but there was a time when downloading a useful JAR or JavaScript file was something you did by hand, so therefore did ad hoc and less often. And I believe it shaped the composition of our code bases. The rise of library use I came to the industry a little after this period, but early enough to work on do medium-sized Java web applications (Apache Struts) where a significant percentage of the overall code was written by me. I (and many other people) did not yet have a culture of trying to download a third party library for every little \"already-solved\" problem. It's not as if I was implementing from machine code up to a full HTTP stack though. The Java standard library is doing a significant amount of heavy lifting, the JVM is abstracting the OS system calls away from me and frameworks like Apache Struts, the Servlet API (and Tomcat), etc. probably end up marginalising my code to a small percentage of the overall application. This is the \"standing on the shoulders of giants\" that is crucial for the software industry to move forwards year on year. However, note that I can just about name all the platforms, frameworks and libraries in a single sentence. In contrast to that, if I print out the dependency tree of a more recent Java project, I see hundreds of packages. It's also a running joke in some circles that the node_modules directory in a typical Node.js project can fill your hard disk. It's not quite that extreme, but I have one project where the node_modules directory totals over 600MB! Another Node.js project I work on professionally has merely 100MB in its node_modules , but that space is taken up by 590 dependencies. This is certainly no longer a list I can recall from memory. Notably, it's also a list where -- on inspection -- I do not even recognise some of the libraries I have supposedly downloaded that are supposedly necessary for my application to run (this is likely not to be true, but that's a whole other topic). Why did this change? The turning point as far as I can see was the moment we automated dependency resolution. For Java projects in the 1990s, you might be willing to downloading a JAR file for a useful library and put it on your CLASSPATH and compile against it. If that library then had other dependencies, then you have to track them down and put those on the CLASSPATH too. This clearly dissuades library developers from having too many dependencies (otherwise the library is too difficult to use) and encourages application developers to solve simple problems themselves rather than search for a library. I had similar experience with JavaScript around the year 2000 in that I was happy to download the odd library like jQuery and sometimes another library (jQuery UI being one of them) would itself depend on jQuery, but downloading two JavaScript files and writing two <script> tags into my page was not too difficult. With Java, Maven changed the cost-benefit ratio of library usage. Today, it would be surprising to see people scatter their code with helper functions for strings or dates when we have Apache Commons, Jodatime, etc. For really difficult domains like time and date manipulation, this is really important as you're delegating (and centralising) effort to cover all the edge cases and domain expertise. But for a simple helper that checks both if a string is null or an empty string at once, this might seem less important, but we bring in Apache Commons anyway as it's almost as cheap as copy-pasting the solutions. How does this affect our code? I think there are two crucial changes this effects in modernising software development: it is possible to write an application with very little code (e.g. microservices) the increase in quantity of dependencies causes great variation in the quality of those dependencies. Allow me to expand a little on both of these. Smaller code bases The most extreme example I can think of where I have built very powerful services with very little code has been when I worked in messsage-based integration projects with ActiveMQ and Apache Camel. For those who have not used it, Apache Camel is a full implementation of Enterprise Integration Patterns in that it provides all the building blocks for reading data from a variety of sources (IMAP, HTTP, ActiveMQ, AWS S3/SNS, etc.), manipulating each chunk or \"message\" of data (transformation, filtering splitting into smaller chunks, aggregating into bigger chunks) and then sending that data to some destination (again, IMAP, HTTP endpoint, etc.). The main logic of your message-routing application is defined in an implementation of their RouteBuilder that is essentially a Domain-Specific Language (DSL) run once at start-up as configuration and now you have a full application capable of solving a business problem, potentially having only written the one Java class. This is because of how much comes out of the box with Apache Camel. Combine this with Spring and a Servlet container like Tomcat and with perhaps only two configuration files (either in Java or XML) you can have a web service that does something useful. Another example is the Django framework in the Python world. Those that use the framework refer to it has having \"batteries included\" due to the amount that it does out of the box. From a simple project skeleton you can have working authentication, a database set up and a full admin interface that lets you manipulate any data models you later define. In fact, the admin interface is so mature and polished that many agencies and freelancers simply skin it nicely and present to clients as the back end inteface for all web apps or web sites they build. The Django community likes to build and share \"apps\" for reuse within the framework, so if I have a need for e.g. some ability to have people rate or vote on some object in my database (for example, voting up and down articles they like/dislike), then I have an app I can pull in and mainly glue into my main project. I think it's clear that you can make very powerful applications today with very small amounts of code. I will cover how this affects unit testing shortly. Quantity vs. quality of dependencies Not only is it cheap to add dependencies to a project and quick to download them, but it is also cheap to publish packages today. A natural effect of this is that we have a long tail of libraries that work fine for our use cases, but are lower in quality than very mature frameworks. This is not a comment on the ability of competence of individual programmers. It is true that less experienced programmers can publish a useful package as part of their learning, but even very skilled developers might quickly wrap up some business logic in a nice package, but not have the time nor incentive to work out all the edge cases or ensure full test coverage. Perhaps this is a shift from big libraries being backed by large companies to individuals being able to compete by fully writing, publishing and maintaining a useful package all without help. Note that also these individuals are so empowered to publish useful open source code because they themselves can pull in dependencies and build value on top of other people's work. And so the cycle continues and I end up with a project with 590 Node.js dependencies. In fact, it seems that this effect is exaggerated in newer ecosystems like Node.js. There is a combined effect of multiple people publishing solutions to the same problem rather than use each other's code (how many implementations of promises are there?) and a culture of pathologically preferring reuse via packages even for simple problems (see: left-pad or isarray). This reduction in average quality of packages combined with our own code shrinking with respect to the amount of code reused I believe requires us to rethink traditional wisdom around unit testing. Unit testing: conventional wisdom The principle behind \"unit\" testing is that you are trying to test only a single function, class or module at a time. The advantage of this is that the moment you introduce a bug to a particular block of code, the tests for that block -- and only the tests for that block -- fail immediately and you know your most recent change must be the one at fault. In contrast integration testing might cover two or more components working together and therefore any test failures indicate potential fault in either component (or both). Conventional wisdom tells us the failure is harder to pinpoint in this case, so it is better to have a unit test fail before we get to that point.","tags":"Software Engineering","url":"https://avengerpenguin.com/mocking-data-only/","loc":"https://avengerpenguin.com/mocking-data-only/"},{"title":"Modular Aesthetic","text":"All the appearance of a modular architecture without any of the benefits.","tags":"misc","url":"https://avengerpenguin.com/modular-aesthetic/","loc":"https://avengerpenguin.com/modular-aesthetic/"},{"title":"Monoids in Python","text":"```python trusted=true from typing import TypeVar, Generic from abc import ABCMeta, abstractmethod T = TypeVar('T') class Monoid(Generic[T], metaclass=ABCMeta): @property @abstractmethod def identity(self) -> T: pass @abstractmethod def compose(self, x: T, y: T) -> T: pass Monoid ```python trusted=false class IntMonoid(Monoid[int]): @property def identity(self): return 0 def compose(self, x, y): return x + y ```python trusted=false class StringMonoid(Monoid[str]): @property def identity(self): return \"\" def compose(self, x, y): return f\"{y}{x}\" ```python trusted=false from hypothesis import given from hypothesis.strategies import data, from_type import pytest from typing import get_args monoids = [ IntMonoid(), StringMonoid(), ] @given(data()) @pytest.mark.parametrize(\"m\", monoids) def test_identity(m: Monoid[T], data): t = type(m.identity) x: T = data.draw(from_type(t)) print(x, m.identity) assert x == m.compose(x, m.identity) assert x == m.compose(m.identity, x) @given(data()) @pytest.mark.parametrize(\"m\", monoids) def test_composition(m, data): t = type(m.identity) x: T = data.draw(from_type(t)) y: T = data.draw(from_type(t)) assert type(m.compose(x, y)) == t @given(data()) @pytest.mark.parametrize(\"m\", monoids) def test_associativity(m, data): t = type(m.identity) x: T = data.draw(from_type(t)) y: T = data.draw(from_type(t)) z: T = data.draw(from_type(t)) xy: T = m.compose(x, y) yz: T = m.compose(y, z) assert m.compose(xy, z) == m.compose(x, yz) import ipytest ipytest.config(addopts=['-q']) ipytest.run() ```python trusted=false from future import annotations from typing import Callable U = TypeVar(\"U\") A = TypeVar(\"A\") B = TypeVar(\"B\") class Functor(Generic[T], metaclass=ABCMeta): @abstractmethod def map(t: T, f: Callable[A, B]) -> T[B]: pass class Maybe(Functor[A], metaclass=ABCMeta): def map(m: Maybe, f): pass class Nothing(Maybe[A]): def init (self, *args): pass def map(self: Maybe, ): return Nothing() def __eq__(self, ): return True def str (self): return '[Nothing]' class Just(Maybe[A]): def init (self, x: A): self.x = x def map(self: Maybe, f: Callable[A, B]) -> Just[B]: return Just(f(self.x)) def eq (self, y): return self.x == y def str (self): return f'[Just({self.x})]' from hypothesis import given from hypothesis.strategies import data, from_type, functions from hypothesis import settings, Verbosity import pytest from typing import get_args functors = [ Nothing, Just, ] types = [ int, str, frozenset, bool, ] identity = lambda x: x @given(data()) @pytest.mark.parametrize(\"t\", types) @pytest.mark.parametrize(\"f\", functors) def test_identity(f: Functor, t: type, data): F = f[t] x = data.draw(from_type(t)) print(F(x), F(x).map(identity)) assert F(x) == F(x).map(identity) @given(data()) @pytest.mark.parametrize(\"C\", types) @pytest.mark.parametrize(\"B\", types) @pytest.mark.parametrize(\"A\", types) @pytest.mark.parametrize(\"f\", functors) def test_composition(f: Functor, A: type, B:type, C: type, data): F = f[A] a: A = data.draw(from_type(A)) def f_spec(x: A) -> B: pass f: Callable[A, B] = data.draw(functions(like=f_spec, returns=from_type(B), pure=True)) def g_spec(y: B) -> C: pass g: Callable[b, c] = data.draw(functions(like=g_spec, returns=from_type(C), pure=True)) def h(x: A) -> C: return g(f(x)) assert F(a).map(h) == F(a).map(f).map(g) @given(data()) @pytest.mark.parametrize(\"D\", types) @pytest.mark.parametrize(\"C\", types) @pytest.mark.parametrize(\"B\", types) @pytest.mark.parametrize(\"A\", types) @pytest.mark.parametrize(\"f\", functors) def test_associativity(f: Functor, A: type, B:type, C: type, D: type, data): F = f[A] a: A = data.draw(from_type(A)) def f_spec(x: A) -> B: pass f: Callable[A, B] = data.draw(functions(like=f_spec, returns=from_type(B), pure=True)) def g_spec(y: B) -> C: pass g: Callable[b, c] = data.draw(functions(like=g_spec, returns=from_type(C), pure=True)) def h_spec(z: C) -> D: pass h: Callable[b, c] = data.draw(functions(like=h_spec, returns=from_type(D), pure=True)) fg: Callable[A, C] = lambda x: g(f(x)) gh: Callable[B, D] = lambda y: h(g(y)) assert F(a).map(f).map(gh) == F(a).map(fg).map(h) import ipytest ipytest.run() ```","tags":"notebooks","url":"https://avengerpenguin.com/monoids-in-python/","loc":"https://avengerpenguin.com/monoids-in-python/"},{"title":"What I am doing now","text":"April 2024 Moving on from my Lead role at INRIX and considering my options for what's next. I've had an interesting year and a half working more hands-on with Terraform and Kubernetes while also guide a team through a DevOps journey. I'm trying to come back to this Digital Garden to write more thoughts and spend more time on a new random project with History of Sound . January 2023 My interest has come back to the idea of a Tech Radar for myself, for my new team at Inrix and in general for the industry. I've created the Tech Radar page on this site to act as a way to write up all the little bookmarks of interesting tools and articles I have. I've done some forks and PRs on Thoughtworks' Build Your Own Radar repo to allow me to publish the visual version within this site. December 2022 Started my new Lead role at INRIX right in the middle of organising Christmas. October 2022 Coming to the end of 12 years working at the BBC and looking forwarding to joining INRIX as a Lead Software Developer. I'm currently revamping my Digital Garden to auto-publish via Github actions and maybe actually polish up some of the notes. October 2021 Having struggling with the friction of writing fully-formed articles for a blog website, I am in the process of transforming this website into a Digital Garden .","tags":"misc","url":"https://avengerpenguin.com/now/","loc":"https://avengerpenguin.com/now/"},{"title":"Obsidian","text":"I came across Obsidian on 2020-09-28 after watching a Ali Abdaal video on The Perfect Note-Taking App .","tags":"misc","url":"https://avengerpenguin.com/obsidian/","loc":"https://avengerpenguin.com/obsidian/"},{"title":"Only work on one thing at a time","text":"TODO","tags":"misc","url":"https://avengerpenguin.com/only-work-on-one-thing-at-a-time/","loc":"https://avengerpenguin.com/only-work-on-one-thing-at-a-time/"},{"title":"Organised Chaos","text":"","tags":"Productivity","url":"https://avengerpenguin.com/organised-chaos/","loc":"https://avengerpenguin.com/organised-chaos/"},{"title":"Oxford Comma","text":"There are cases where it makes things worse: The student thanked her principal, Kanye West, and her little brother. From The Case Against the Oxford Comma .","tags":"Language","url":"https://avengerpenguin.com/oxford-comma/","loc":"https://avengerpenguin.com/oxford-comma/"},{"title":"Philosophy","text":"Schools of interest: Stoicism Cynicism Marxism Psychological types Epistemology Scepticism Feminism","tags":"Philosophy","url":"https://avengerpenguin.com/philosophy/","loc":"https://avengerpenguin.com/philosophy/"},{"title":"Platform Engineering","text":"type: folder_brief_live","tags":"Platform Engineering","url":"https://avengerpenguin.com/platform-engineering/","loc":"https://avengerpenguin.com/platform-engineering/"},{"title":"Platform Pitfalls and Myths","text":"When building an internal platform for an organisation, there are a few traps people fall into. The breakdown of this is largely thanks to Scaling your business with platform thinking by Thoughtworks with details filled in with my own experience with successful and unsuccessful platform initiatives. For how to get it right, see Building a Successful Platform . 1. Build it and they will come This is arguably an extension of product iniatives in general without fully understanding customer need or establishing effective feedback loops. Sometimes we can be so sure we see a need for a platform or framework we presume people will simply naturally want to come use it. This can be a combination of Endowment effect and Escalation of commitment (aka Sunk costs fallacy) leading us to value the platform more than its potential customers. There is a nonzero overhead and cost for customers to discover, understand and learn how to adopt your platform. That cost might even be higher than a team proceeding (at least initially) to build the capabilities themselves (especially when they already have the knowledge to do so). I prefer to consider it in terms of Ecomonics or a market -- what is the incentive to use your platform over other options? This is very clear in open source tools -- users will adopt a framework when it clearly makes their development experience better and avoid them when the cost of adoption is too high. 2. Mandate to increase adoption A natural follow-on from above is to use a high-level mandate within the business to force adoption of a platform. This is especially common in reaction to hitting the reality of the \"build it and they will come\" myth where in fact, they do not automatically come. Again, arguing from Economics or indeed human behaviour, we can clearly empathise with the idea that teams will not willingly adopt a platform that only adds coupling to other teams or friction to their workflows. 3. Platforms help you beat Conway's Law Conway's Law states that \"Organizations, who design systems, are constrained to produce designs which are copies of the communication structures of these organizations.\" Common outcomes of this include systems not being joined up if there is too much communication friction. A temptation here is to build a platform that encourages everyone to collaborate, but organisational friction should be fixed by team structures and topologies. A platform cannot magically fix organisational friction (and again, the temptation is to mandate adoption when existing friction prevents it happening naturally). 4. Platforms are monoliths Another temptation is to build platforms as a singular monolith with a highly-abstract API and a single use case. In reality, successful platforms are a set of loosely coupled capabilities where teams can perhaps adopt it partially where they have solutions for some functionality already. Or theyy may wish to orchestrate the capabilities in newer ways. The capabilities should be arrange to create a meaningful experience, built around use cases as they emerge.","tags":"Platform Engineering","url":"https://avengerpenguin.com/platform-pitfalls-and-myths/","loc":"https://avengerpenguin.com/platform-pitfalls-and-myths/"},{"title":"Premature scaling can stunt system iteration","text":"This is a phrase I've lifted entirely from Andy Matuschak's notes which I've kept verbatim as it's a nice, punchy way of tying together various concepts I see as mistakes and anti-patterns in product and sofware development. I see this as a summary various things that paint you into corners if you do not following them (or at least understand them): YAGNI DevOps as a special case of leveraging feedback loops","tags":"misc","url":"https://avengerpenguin.com/premature-scaling-can-stunt-system-iteration/","loc":"https://avengerpenguin.com/premature-scaling-can-stunt-system-iteration/"},{"title":"Productivity","text":"Random topics: Four Burners Theory Agile Results ENTP Productivity Explore-Exploit Dilemma","tags":"Productivity","url":"https://avengerpenguin.com/productivity/","loc":"https://avengerpenguin.com/productivity/"},{"title":"Proving currying","text":"Showing equivalence between the forms: f: (a, b) -> c g: a -> b -> c Require Import Coq.Init.Datatypes. Parameters A B C : Set. Definition curry (f: A * B -> C) := fun a => fun b => f (a, b). Definition uncurry (g: A -> B -> C) := fun p => g (fst p) (snd p). Theorem inv : forall f a b, uncurry (curry f) (a,b) = f(a,b). Proof. intros. unfold curry, uncurry. simpl. reflexivity. Qed. Theorem inv2 : forall g a b, curry (uncurry g) a b = g a b. Proof. intros. unfold curry, uncurry. simpl. reflexivity. Qed.","tags":"notebooks","url":"https://avengerpenguin.com/proving-currying/","loc":"https://avengerpenguin.com/proving-currying/"},{"title":"Python speeds up feedback loops","text":"","tags":"misc","url":"https://avengerpenguin.com/python-speeds-up-feedback-loops/","loc":"https://avengerpenguin.com/python-speeds-up-feedback-loops/"},{"title":"Semantic Arguments","text":"As a species, we like to debate a lot. There are debates over politics, religion, music, art, films, programming languages, ethics, favourite \"My Little Pony\" character and many more important topics. One of my favourites in the web engineering world is debates about what \"REST\" is or is not . While I argue there is value on understanding what REST was originally defined to be, I'm probably inviting a debate with arguments such as \"REST isn't that, it's this\" or semantic debates about whether a given system \"is RESTful\" or not. What's missing in that hypothetical argument? Something that's missing in a lot of discussion on the web about this: any concrete discussion of whether properties of the thing being discussed are good or useful. We are just focusing instead of whether things qualify for a label and possibly not even acknowledging that all sides involved could be using different definitions for that label. This happens in multiple fields: Is some musical genre \"real\" music? Is modern art really art? Is this leader truly left wing? Is this programming language really a functional language? The debate has broken down to arguing the ontological commitments made by assigning something to a class or category, rather than discuss the thing originally being debated. Consider the productive discussions we could have with each of these examples: What musical features are present in this genre of music? How do different people respond to it? Do people like modern art? What patterns have emerged that differentiate it from more traditional art forms? What do people feel when they see it? What policies does that leader want? What are her values? How does his beliefs match up to past leaders and famous figureheads? Does this programming language allow immutable data structures? Are they useful in my application? What bugs are hard to write in this language? These are arguably a more epistemological debate around what we know about these nuances, how our opinions are justified and what beliefs are reasonable and rational. We are also viewing politicians -- to use that example -- through a lens where we judge people on their actions, policies and effectiveness thereof, rather than on whether they are \"truly left wing\" or \"a true Conservative\". Don't accept someone saying they do not like a programming language because, for example, it is \"not truly functional\". Ask instead to debate the merits of various features. Perhaps also ask to focus on the objective qualities of a programming language. As I've covered in both REST and programming language, the semantics and proper use of labels should not be ignored. They are a useful proxy and shortcut for getting across our meaning. If I say a web service is not \"RESTful\" (i.e. does not follow the REST architectural style) then I'm taking a shortcut for talking about how it's coupled to its clients or too reliant on out-of-band information for integration, which couples it further. Where we need to be careful is that arguments cannot progress if we argue over labels without unpacking what definitions we are using for those labels. If we fundamentally disagree on what a label means, then perhaps we need to drop semantics altogether and talk only about properties and nuances for a while. When someone says \"this is not music\", they are telling you they do not like it. A reasonable person might be willing to expand on it further or you might uncover prejudices that led to their knee-jerk dismissal of that genre. Again, the debate cannot progress if two sides disagree on the definition of music, but discussion on why our definitions differ would be interesting. If we call a leader \"not left wing\" or \"not a real Conservative\" we are likely alluding to specific policies or actions with which we do not agree. Maybe we can discuss those further? Maybe we can recognise that even in a right wing government, some traditionally left-wing policies make sense in the modern world? Perhaps these ideas are not so imcompatible? I assert that behind any semantic discussion is a more useful debate waiting to happen. So, next time someone tells you some music \"isn't music\" or some art \"isn't real art\", instead of justifying why they do fit those labels, ask instead what they like and do not like about it. Maybe even discuss why people like different kinds of art and music. Not only might we learn a lot more, but we might even get along a lot more.","tags":"misc","url":"https://avengerpenguin.com/semantic-arguments/","loc":"https://avengerpenguin.com/semantic-arguments/"},{"title":"Shipping Theatre","text":"I first heard this term from James Stanier, Director of Engineering at Shopify . As with Security Theatre, we can consider Shipping Theatre to be an over-reliance on the superficialities of shipping product features, but in such a way there is no real impact. The idea is that a team can only ship the right things with clear metrics and a clear measure of their impact. In the absence of this, teams resort -- perhaps unknowingly -- to focus on just shipping anything. Teams focus too much on the deliverable, not what the deliverable is intended to do. Shipping and big launches can be celebrated, but don't know whether the launches were worth doing.","tags":"DevOps","url":"https://avengerpenguin.com/shipping-theatre/","loc":"https://avengerpenguin.com/shipping-theatre/"},{"title":"Socialist Articles","text":"Misc articles around Socialism and related issues. There are plenty of Palestinian Gandhis. Israel keeps killing them","tags":"misc","url":"https://avengerpenguin.com/socialist-articles/","loc":"https://avengerpenguin.com/socialist-articles/"},{"title":"Socio-technical Systems","text":"","tags":"DevOps","url":"https://avengerpenguin.com/socio-technical-systems/","loc":"https://avengerpenguin.com/socio-technical-systems/"},{"title":"Soft Discipline","text":"An approach to self-discipline where: we avoid strict planning we avoid micro-managing via a fixed schedule rely on deadlines \"Powering through\" Avoid powering through work you're not enjoying or struggling with. If it's hard, consider if you need to step back and act like a beginner in the task and go back to being playful. Personal Tax Stop beating yourself up for the \"tax\" of the downside of your personal nature. Instead just accept it as a cost of your strengths from that same nature. For example, someone impatient and novelty-seeking will struggle with rushing things or impulse-buying, but that same trait makes that person quick to pick up new things and open to trying new things. Compensation If you are making yourself work hard at something, are you compensating yourself enough for it? Are you reaping the benefits and actually enjoying them? Are you taking breaks by doing other things for a bit so you're not flat-out?","tags":"misc","url":"https://avengerpenguin.com/soft-discipline/","loc":"https://avengerpenguin.com/soft-discipline/"},{"title":"Software Engineering","text":"Languages Ones I know to varying degrees, In no particular order: Python Java Clojure Prolog JavaScript PHP Ruby Processes I have a lot to say about: DevOps Kanban Agile Continuous Delivery Continuous Improvement","tags":"Software Engineering","url":"https://avengerpenguin.com/software-engineering/","loc":"https://avengerpenguin.com/software-engineering/"},{"title":"Software is a service industry not a manufacturing industry","text":"","tags":"misc","url":"https://avengerpenguin.com/software-is-a-service-industry-not-a-manufacturing-industry/","loc":"https://avengerpenguin.com/software-is-a-service-industry-not-a-manufacturing-industry/"},{"title":"Staying in the Garden","text":"When we stop to smell the roses, we notice things we did not see before. Imagine you are a gardener maintaining a handful of public and private gardens in your area.","tags":"misc","url":"https://avengerpenguin.com/staying-in-the-garden/","loc":"https://avengerpenguin.com/staying-in-the-garden/"},{"title":"Stoicism","text":"Stoicism is a philosophy that focuses on avoiding destructive emotions and living a virtuous life worrying about only what you can control. Some thoughts in this area: Impermanence What we control and what we don't Less is More","tags":"Stoicism","url":"https://avengerpenguin.com/stoicism/","loc":"https://avengerpenguin.com/stoicism/"},{"title":"Storytelling techniques for creating impactful presentations","text":"Public speaking is not just speaking in public. It is storytelling. -- Arquay Harris Foundation Message \"What do I want people to walk away knowing/thinking/feeling?\" Anticipate what might prevent that. Tone Is this talk funny? Serious? Informative? Match tone to the message. Don't use humour if announcing redundancies. Audience To whom am I talking and what are their priorities? Story Structures Beginning Hardest part. Find your narrative arc Define beginning and end Then fill in the middle Hero's Journey/Monolith Hero leaves home for a difficult journey. Comes home with wisdom. \"My journey from high-school dropout to CEO of a fortune 500 company\" The Mountain Like hero's journey, but not always a happy ending. Building tensions, overcoming obstacles. \"I was born to a single mother in rural Mississippi and my highest aspiration was to one day be a maid like my grandmother\" - Oprah Nested Loops Three or more narratives layered within each other. Core story at the centre and others elaborate on it. First story you begin is the last you finish. \"Growing up, I faced many obstacles... I had a 2nd grade teacher who taught me... What I learned was...\" Titanic: Old woman Jack and wossname Sparklines Structure often contrast ordinary with extraordinary/ideal. Steve Jobs: unveils iPhone. Talks about clunky reality of phones and paints a picture of what it could be. Inspiring the audience to action Creating hope and excitement Creating a following \"Imagine a world where deploys take 2s... What if tests took only 30s to write...\" MLK \"I have a dream\" -- contrasting dream world with reality. In Medias Res Narrative begins in the heat of the action, then starts over at the beginning. Good for short presentations. Grabs attention from the start. \"I never thought one semi-colon would cause the destruction of my company...\" Converging Ideas Structure that shows the audience how different strands came together. \"How did a philosopher, an English hacker, a CSS MAster and a db wizard come together to create of the biggest software success stories? Twice.\" False Start Stat telling a predictable story, then disrupt. Disrupt expectations Show benefits of a flexible approach Keeping audience engaged \"I raised 15M in funding. Assembled a world class team. And after two years of hard work launched a visually stunning video game. But this isn't a story about success. This is a story about failure.\" Petal Structure Organise multiple stories around central concept. Show how strands are interconnected Showing how several scenarions relate back to one idea Uses repetition like as with Sparklines. What is Slack so successful? One of a kind innovation (IRC, Hipchat) Brilliant founders? (MS, Google) Good timing? (Palm, Yahoo) Key takeaways Find your purpose \"If someone walked over to your desk and asked you to explain what you were working on, could you?\" If you can't explain it simply, you don't understand it. You can be confident if it's aligned with your purpose. Find your emotions \"The best speakers speak from the heart. It is not about what they say, it is about how they make you feel.\" Find your composure Nerves are normal.","tags":"misc","url":"https://avengerpenguin.com/storytelling-techniques-for-creating-impactful-presentations/","loc":"https://avengerpenguin.com/storytelling-techniques-for-creating-impactful-presentations/"},{"title":"Tech Radar - Languages & Frameworks","text":"Languages & Frameworks These are languages and frameworks on my radar at various levels. See the top-level Tech Radar for other categories. Note that I rarely look at whole new languages as I have a bias towards frameworks and libraries that help me build things over improving the act of writing code itself. A language might have promise on here if its ecosystem of libraries and frameworks mature around it since that's a key feature for me over aethetics of syntax or language semantics. Quarkus Status: Assess It's clear Quarkus is inevitible if Java is to survive the \"cloud native\" trends that see people otherwise favour Go or other things that compile to a small binary that boots quickly in a small container. Sanic Status: Assess Sanic is a Python web server and web framework that's written to go fast. See Sanic on Github . Quart Status: Assess See Quart docs . IDOM Status: Hold It's React, but in Python! While interesting, I'm yet to think this is a good idea over, say, htmx. See IDOM docs htmx Status: Trial A solid attempt to extend HTML affordances in a standard way user agents could well support natively in future. I think this is a strong approach than bloated UI frameworks like React. See htmx - high power tools for html . Typer Status: Adopt I'm a big fan of libraries that use decorators etc. effectively so you can write your code as you normally would and it \"just works\" to turn it into e.g. a CLI tool as is the case with Typer. There's an extra appeal with it using type hints to do some magic that other libraries like clize need help with. I certainly don't know why anyone would continue using ArgumentParser in Python today. See Typer . OpenTelemetry Status: Trial Still looking for a way to bring together all the tools I've used in this space to get the most out of this standard.","tags":"Tech Radar","url":"https://avengerpenguin.com/tech-radar-languages-frameworks/","loc":"https://avengerpenguin.com/tech-radar-languages-frameworks/"},{"title":"Tech Radar - Platforms","text":"Platforms Solid Status: Assess See Solid Project Website Gemini Status: Hold An alterntive to the World Wide Web and HTTP for hypermedia pages. See Project Gemini for a full explanation. Airflow Status: Assess A reasonably plug-and-play platform for constructing data flows as directed, acyclic graphs all in Python. This seems favourable to data scientists who are already experienced in Python due to Jupyer et al. See Apache Airflow .","tags":"Tech Radar","url":"https://avengerpenguin.com/tech-radar-platforms/","loc":"https://avengerpenguin.com/tech-radar-platforms/"},{"title":"Tech Radar - Techniques","text":"Techniques These are software engineering techniques on my radar at various levels. See the top-level Tech Radar for other categories. Platform Engineering Status: Assess At the time of writing, this is a fairly emergent term that has a few people excited although -- as it typically the case -- it's a term for a pattern that has been growing for some time. See Building a Successful Platform and Platform Pitfalls and Myths for some more of my thoughts on platforms. See What is Platform Engineering? for an introduction. Data Mesh Status: Assess An alternative to centralised data lakes -- i.e. what if we could distribute data across an organisation such that people locally own the parts that are relevant for them to own, but we can still do analysis and mining across it all. Shape-up Status: Trial This is probably best explained in the fully Shape-up book published by Basecamp. A reductive summary is that it's a process of working in larger \"blocks\" or sprints of time (e.g. 6 weeks) and choosing work for those sprints based on things you think are worth spending that time on. A key component of this is stopping if the time is exhausted to avoid the sunk cost fallacy.","tags":"Tech Radar","url":"https://avengerpenguin.com/tech-radar-techniques/","loc":"https://avengerpenguin.com/tech-radar-techniques/"},{"title":"Tech Radar - Tools","text":"Tools git-xargs Status: Adopt Do commands across multiple Github repos at once. See gruntwork-io/git-xargs: git-xargs is a command-line tool (CLI) for making updates across multiple Github repositories with a single command. Testmon Status: Assess See About | testmon Diagrams Status: Adopt In the spirit of PlantUML, a Python library for generating diagrams with specific icons. What appeals in particular is the C4 shapes as well as cloud-provider-specific icons. See mingrammer/diagrams: Diagram as Code for prototyping cloud system architectures gitStream Status: Assess A tool to push the idea of \"continuous merge\". See gitStream by LinearB ¬∑ GitHub Marketplace . Gatling Status: Adopt JVM-based load testing tool. nitpick Status: Assess Potentially useful for enforcing standards across multiple repos. tfautomv Status: Assess Looks very useful for big refactors but the need for it hasn't quite come up yet for me. See busser/tfautomv: Generate Terraform moved blocks automatically for painless refactoring .","tags":"Tech Radar","url":"https://avengerpenguin.com/tech-radar-tools/","loc":"https://avengerpenguin.com/tech-radar-tools/"},{"title":"Tech Radar","text":"Technical projects, blogs or other ideas that are on my personal \"tech radar\" as something I may or may not come back to. To follow conventions from the Thoughtsworks Tech Radar , I have broken this down into four areas: Tech Radar - Tools Tech Radar - Platforms Tech Radar - Techniques Tech Radar - Languages & Frameworks There is also a visual version using the \"Build Your Own Radar\" tool from Thoughtworks (with some hackery to publish it here).","tags":"Tech Radar","url":"https://avengerpenguin.com/tech-radar/","loc":"https://avengerpenguin.com/tech-radar/"},{"title":"Technical Debt","text":"Technical Debt is a very broad term, mostly used by engineers to refer to anything they'd like to change at some point. Definition I prefer an interpretation closer to the original \"debt\" metaphor it leverages: Technical debt, like financial debt, is any liability in your project that costs you over time and would incur costs in \"paying it off\" This includes bad design, bad builds, bad docs, bugs and a whole lot more, but crucially only things that actual have a cost. With this definition, we get to make the same trade-off decisions we would with financial debt: If the debt is low cost (low interest i.e. it doesn't cost you much to retain it), then you likely want to pay it off slowly over time as with a mortgage or indeed happily never pay it off If the debt is high interest/cost (it's costing you every day), then it likely needs you to stop paying into new features and pay the debt off instead. This follows nicely from the financial analogy where very high interest debts should probably be a priority over buying new, non-critical things. Then we also get to factor in just how much it will take to clear these different scales of debt. We also get to consider when we need to call \"bankruptcy\" and start again. Note that large companies happily take on huge amounts of financial debt as a matter of course if it's helping leverage more returns elsewhere. This is a key insight engineers may want to consider since it can be tempting to try to fix everything eventually, but we may wish to borrow this idea from finance that perpetually living with debt in business can be normal and not an aberration. Classification There are two different categorisations I have found that I like for reasoning about technical debt (and therefore treating different categories differently): In Understanding Technical Debt , Michael Portwood lays out a classification based on the impact of the debt. I think this key as each category needs its own treatment and plan. It also follows that it might be worth classifying debt we find before fixing it right away. Back in 2009, Martin Fowler published a simpler TechnicalDebtQuadrant to show a similar breakdown, highlighting that some debt is prudent, not always reckless. In Towards an Ontology of Terms on Technical Debt , the authors describe an ontology based on the technical aspects of the debt. Such a breakdown might be most useful when trying to address the debt.","tags":"Software Engineering","url":"https://avengerpenguin.com/technical-debt/","loc":"https://avengerpenguin.com/technical-debt/"},{"title":"Technical Design Documents","text":"A technical design document is a useful way to lay out details of a solution before it is implemented in code. A key feature for me is the ability to describe trade-offs made so it is clear that alternatives are acknowledged but -- while the problem space is the way it is -- the specific approach described is chosen for now. This implies that a good document will show enough working such that if the situation does change, we might reconsider some decisions. This borrows nicely from the scientific method in that we are writing things in a way that openly invites falsifiability. Further reading: Staff Engineer: Leadership beyond the management track: Amazon.co.uk: Larson, Will, Reilly, Tanya: 9781736417911: Books Scaling Engineering Teams via RFCs: Writing Things Down - The Pragmatic Engineer Design Docs, Markdown, and Git ‚Äì Caitie McCaffrey Design Docs at Google Technical Decision-Making and Alignment in a Remote Culture | Stitch Fix Technology ‚Äì Multithreaded","tags":"Software Engineering","url":"https://avengerpenguin.com/technical-design-documents/","loc":"https://avengerpenguin.com/technical-design-documents/"},{"title":"Technical Strategy","text":"Further Reading: A Framework for Responsible Innovation | Stitch Fix Technology ‚Äì Multithreaded Good Strategy/Bad Strategy: The difference and why it matters: Amazon.co.uk: Rumelt, Richard: 9781781256176: Books How Big Technical Changes Happen at Slack - Slack Engineering","tags":"misc","url":"https://avengerpenguin.com/technical-strategy/","loc":"https://avengerpenguin.com/technical-strategy/"},{"title":"Technical Vision","text":"The technical vision or mission that has been guiding my career could be summed with \"Faster Feedback Loops\". This comprises a few principles: Culture of experimentation Being data-driven Optimised lead times Frequent releases Fewer remits per team, fewer teams per remit Quick hand-offs Less context switching","tags":"misc","url":"https://avengerpenguin.com/technical-vision/","loc":"https://avengerpenguin.com/technical-vision/"},{"title":"The Open Source Hive Mind is smarter than you","text":"It is.","tags":"misc","url":"https://avengerpenguin.com/the-open-source-hive-mind-is-smarter-than-you/","loc":"https://avengerpenguin.com/the-open-source-hive-mind-is-smarter-than-you/"},{"title":"The Perfect Note-Taking App","text":"Notes on the following video The Perfect Note-Taking App by Ali Abdaal. Personality Archetypes The mention of choosing based on personality archetypes was particularly interesting to me. This is based on a Ness Labs article on choosing the right note-taking app .","tags":"misc","url":"https://avengerpenguin.com/the-perfect-note-taking-app/","loc":"https://avengerpenguin.com/the-perfect-note-taking-app/"},{"title":"The best software has the fewest moving parts","text":"Dave Farley has said at various times \"the ability to change your software is the is the defining characteristic of its quality\". This comes up for example in The Ultimate Quality Of GOOD Software . Combining this idea with a sprinkling of Occam's razor we get a principle that we should largely favour less in software engineering wherever possible.","tags":"misc","url":"https://avengerpenguin.com/the-best-software-has-the-fewest-moving-parts/","loc":"https://avengerpenguin.com/the-best-software-has-the-fewest-moving-parts/"},{"title":"There is no such thing as the individual","text":"Inversion of \"there is no such thing as society\".","tags":"Philosophy","url":"https://avengerpenguin.com/there-is-no-such-thing-as-the-individual/","loc":"https://avengerpenguin.com/there-is-no-such-thing-as-the-individual/"},{"title":"Things I Have Made","text":"This is a overview of various projects I am currently working on or have built in the past. Pokemon Guessing Game Play the game . Voltaire View Github repository This is a bit of a \"glue\" project for the Pelican static site generator to provide a set of defaults and hooks to automatically \"plug in\" useful add-ons like search, google ads, etc. based on config. The intent here is to provide minimal bootstrapping between creating a new vault in Obisidian (or indeed any method of managing a directory of markdown files) and a fully-feature static site. The mission is to reduce overheads such that it is possible to manage multiple sites, apply broad updates across them all but still be able to configure things differently in each one where they need to diverge. Weblint View Github repository A project wrapping Scrapy to provide a way to crawl and \"lint\" a whole website. The primary aim at first was to look for broken links but the vision is to create a full test suite for static sites which will pair nicely with Voltaire above. Hyperspace General-purpose REST and hypermedia client written in Python. Implements support for multiple hypermedia types using Mike Amundsen's H Factor model.","tags":"misc","url":"https://avengerpenguin.com/things-i-have-made/","loc":"https://avengerpenguin.com/things-i-have-made/"},{"title":"Thoughtworks Tech Radar 27","text":"Notes and thoughts about the 27th volume of the Thoughtworks Tech Radar. Techniques Hold Satellite workers without \"remote native\". Remote workers are starting to be exluded again from meetings etc. as we move into a more hybrid model. With remote workers, it's easy to add more people than is necessary to meetings. People remoted into a meeting room can't always hear everything being said. Superficial cloud native. There's a return to the \"lift and shift\" of deploying monoliths on top of cloud vendor services as opposed to rethinking to maximise the value of cloud architecture. Elasticity is a key feature of true \"cloud native\" Thought experiment: can I turn it off automatically if it's not under load? Assess CUPID A follow-on from \"SOLID\" Dan Terhorst-North's attempt to describe \"properties\" over \"principles\" Composable: plays well with others Unix philosophy: does one thing well Predictable: does what you expect Idiomatic: feels natural Domain-based: the solution domain models the problem domain in language and structure Properties lead to \"joyful\" coding. Adopt Team Cognitive Load Comes from \"Team Topologies\" Measure the amount of knowledge needed for a team to get things done For example, a \"front end\" team might need to understand the whole business if they're the front layer to everything else Seems to feed better into cross-functional, vertical teams that own a domain slice, but across multiple technologies (e.g. microservices) Tools Trial Excalidraw Online, collaborative whiteboard Makes diagrams look rough and hand-drawn xbar for build monitoring Build failures on the Mac menu bar Assess Teller Puts secrets in environment variables Uses multiple sources Hold Online services for formatting or parsing code Using online JSON/YAML formatters Pasting potentially sensitive info into 3rd parties JSON from log files could have personal information -- would be a GDPR violation or even exporting of personal data if the service is hosted in another country Platforms Adopt Backstage Facilitates finding documentation for APIs (API Catalogue) Uses Markdown TechDocs Delegates writing of docs to the local repo, but centralises the discovery Can enable one-click quickstarts for projects Includes Kubernetes monitoring and management (from a service view, not a K8s admin view) There's quite a lot to it so it needs some management for rolling it out Assess Dragonfly Alternative to Redis Uses new ring buffer algorithms Still uses LRU for evictions Provisional cache mechanism avoids high churn and thundering herd after outage OrioleDB New storage engine for PostgreSQL Designed for SSD and NVRAM (original Postgres engine optimised for hard drives) IAM Roles Anywhere Get temporary credentials to AWS services from outside of AWS Languages and Frameworks Assess JonRunr Alternative to Quartz Job runner with built-in dashboard Schedule tasks and cron jobs via lambda expressions Stable Diffusion Generates images from text prompts like DALL-E Raises more questions around the ethics of using artists' work and style More broadly, crowd-sourcing data with permission raises questions Carbon Aware SDK Helps software to use less carbon Adopt Kotest Previously KotlinTest Sensible default over JUnit Property testing built in Fluent DSL Rich matchers","tags":"Software Engineering","url":"https://avengerpenguin.com/thoughtworks-tech-radar-27/","loc":"https://avengerpenguin.com/thoughtworks-tech-radar-27/"},{"title":"Toxic Productivity and ADHD","text":"Inspired by Avoiding Toxic Productivity Advice for ADHD: Find What Actually Works Productivity systems like Getting Things Done or 43 Folders are designed for their authors. A big flaw in most of these traditional systems is that they reduce anxiety by creating trust that you will get to things. This is difficult for people with ADHD who might not trust themselves like that. Three additional flaws: Most systems require motivation and for ADHD people, long term rewards, etc. don't motivate in a neurotypical way Distrations are bad for people with ADHD and most productivity systems come loaded with distrations with all the bits you can tweak and tune Overwhelm is an issue where if you gather everything in one place, the brain can shut down and it's near impossible to make the choice to make progress The example of \"eat the frog\" (do the really hard thing first) can be dangerous if it causes you to get blocked all day not able to start that thing. Breaking a big project into steps sounds logical, but it increases the overwhelm as the project seems to grow to an infinite size. So, instead of the neurotypical motivations of: Important Consequences Rewards People with ADHD may wish to consider instead: Captivate -- Something that captures your interest. Create -- Something novel you want to dive into and provides dopamine. Compete -- A good challenge or a competition with someone else. Someone telling you that you can't do it can be a great motivator. Complete -- Deadlines, urgency, panic. Strategies to use according to Jessie J. Anderson: Embrace the pivot (captivate, create) -- Accept any new system is going to fail and you will abandon it, therefore simply do not pour everything into it; use it lightly so you are ready to pivot to the next system Pomodoro Timers -- Create short deadlines with burst of 20-40 mins of work Look for sidequests -- Being careful not to get derailed, you can still find a side angle on a project that is more interesting. Micro commitments -- Commit to small amounts of progress only Change your environment -- simply working somewhere different can help (e.g. coffee shop) Making tests or paperwork into a game -- Do questions in reverse order, mix it up in other ways, etc. Make time-based goals -- Instead of \"write 1000 words\", say \"write for 20 mins\"; instead of \"clean the whole office\", say \"I'll spend 10 mins cleaning\"","tags":"Productivity","url":"https://avengerpenguin.com/toxic-productivity-and-adhd/","loc":"https://avengerpenguin.com/toxic-productivity-and-adhd/"},{"title":"Undifferentiated Heavy Lifting","text":"In a microservices architecture, it is inevitable that there will be duplication of work between different product teams. It is a somewhat necessary overhead that each team will have to do their own deployments, pipelines, automation and other fundamental work as a trade-off for having autonomy over their own product or service. Therefore we must keep in mind two key things when considering a microservice approach: Do we understand where and when to use microservices, what they are for and -- crucially -- when not to use them? How can we manage to minimise wasted effort on duplication in this \"heavy lifting\" work common to all microservice product teams? Naive Solutions I use the term \"naive\" in a sense that fits well with Computer Science -- that is, a naive solution or approach is the \"obvious\" one you start with as a first pass and then try to improve. This is similar to the red hat in Edward de Bono's Six Thinking Hats -- it's a perfectly valid starting point, but then we can apply different thinking and do better. Return of the Monorepo/-ilith One instinctive reaction I have seen was the usual \"hype curve\" behaviour. Teams adopt microservices without truly understanding what they were for, hit all the issues with undifferentiated heavy lifting ramping up rapidly and then the natural reaction is to swing back to monolithic approaches, but strangely not necessarily full monoliths. There are a fair few trends at the moment to attempt to retain the Modular Aesthetic in a Mono repo but either still deploy the parts independently or have some tooling to manage reassembling the parts into a single artefact. The former gives you the worst of both worlds in that code changes across deployable parts are possible, but then deployments will not be atomic which could be problematic unless we specifically code or test for the interim state where the change is only briefly applied in one place before the second deployment finishes. The latter -- deploying a single artefact -- avoids this but has a lot of complexity over simply organising code into directories. It is possible I have not fully understood the benefits. In either case, this is a shift away from microservices which is good in cases where you have identified that microservices were not the right solution (yet?) but the \"naivety\" comes in if that shift to monoliths (or monorepos) is happening as a reaction to the undifferentiated lifting -- especially if it's reacting to a perception that duplication is an issue without measuring that it is indeed an issue. In-house Frameworks Another reaction to duplicated heavy lifting between teams is to try to create an in-house platform or framework on which everyone must build their services. The noble attempt is to create an abstraction that deals once with lower level concerns and then teams \"just\" write their business-level logic on top. There are so many issues that can arise from this, it services better as a quick last to expand upon later: The Inner-platform-effect kicks in and we end up with poor replica of platform APIs provided directly by a Cloud provider. Something that arises from the previous point -- but also in other ways -- is that any in-house platform or APIs will require bespoke clients and command-line tools to create abstractions on par with open source tools like awscli or terraform . When your deployments are optimised for one approach, it may make it more expensive for teams that simply need to deploy and build things differently for valid reasons. For example, there's little they can likely share in terms of tooling hard-wired for the in-house platform. It is easy to create single points of failure such that a bad deployment of the platform itself takes out your whole website or application, losing a key benefit of microservices -- particularly micro front ends -- where we get failure \"bulk heads\" that isolate outages to that part of the system. Managing Duplicated Heavy Lifting So, if the previous approaches are \"naive\", how do we manage undifferentiated lifting within a microservice architecture? Start with Monoliths A good way to avoid lots of duplication is not to adopt microservices right away. This has all the benefits of retaining a monolith but notably avoids some of the \"naive\" solutions above where there are monolithic approaches that naively attempt to retain the modularity of microservices . As stated earlier, this appears to be a habit to retain a Modular Aesthetic -- where people like the \"feel\" of things being broken up without necessarily figuring out the best way to carve up and how best to do it. Since Premature scaling can stunt system iteration , a strong way to figure how the \"best\" way is to defer it and get there iteratively. That is, start with a monolith service, avoid complex monorepo tooling and resist the urge to abstract out libraries, frameworks, etc. There is much to read on this such as the Monolith to Microservices book and this allows a long term strategy along the lines of: Build a monolith system Find clear \"seams\" to break out some functionality into distinct services Note where you have to copy-paste code/scripts for build automation and deployment Accept there is some duplication for a while Over time -- and one at a time -- see if there's a robust way to abstract out build and deployment logic What does that abstraction look like over time? Abstracting Automation From observation and hands-on experience, I can see a set of clear \"rules\" (maybe guidelines) worth considering when trying to create good abstractions that reduce duplication between microservice teams: Iterate -- first and foremost, we must emphasise that we won't solve all of it at once and up front. Without real world experience of the services we plan to build, we can't possible sink in up-front time to building all our tooling. In the most ideal cases, I have been able to have essentially zero explicit time put aside for developing tooling and have let it emerge while delivering business features. Open Source First -- before writing any code, it is worth thinking about how to be lazier about it or how to rely more on highly-maintained tools that already exist. Some might even get fun building their own tools to solve a problem and I think this drive falls out of that doing so in your own time is a great way to learn, but \"Total Cost of Ownership\" includes maintenance costs and not just the time it took to build. A custom script that took minutes to write could still be more expensive than a known CLI tool, a Jenkins plugin, a docker container, etc. Inheritance over Composition -- perhaps a controversial way to phrase it (which I do to make people pause and think) but I argue that the Unix Philosophy tells us to make tools with sensible defaults, so a way to achieve this is to construct tooling such that each service \"inherits\" from a central place with local control to override. This is in opposition to having libraries of sharing functions but we still duplicate the coordination scripts that call those functions. That is, we should be making share tooling that encodes flow and cedes control to the abstraction as opposed to simply wrapping up subroutines in function calls.","tags":"DevOps","url":"https://avengerpenguin.com/undifferentiated-heavy-lifting/","loc":"https://avengerpenguin.com/undifferentiated-heavy-lifting/"},{"title":"Voltaire","text":"See docs .","tags":"misc","url":"https://avengerpenguin.com/voltaire/","loc":"https://avengerpenguin.com/voltaire/"},{"title":"What is DevOps?","text":"A common problem with any buzzword coined in the software industry is that we struggle to pin down a precise definition. This is almost certainly the case with DevOps with multiple companies, blogs and experts using the term to mean different things. With a systems mindset, all the definitions out there are arguably all correct at some level, with the \"true definition\" actually dependent on what level of abstraction appropriate to the situation. My definition I find it useful to use a definition of DevOps (or indeed any other term) that is the most general while still being precise enough so we are clear what things fall under that term and what things do not. To that end, my preferred definition of DevOps is: Any practice, technique or tool in the software industry that is intended to speed up delivery of software changes. First, allow me to unpack some of the parts of that definition and then I will explain why the definition around the intent to \"speed up\" is the correct focus. Finally, I will discuss how various practices and implementations we may be familiar with fit nicely within the resulting \"DevOps\" umbrella that follows from this definition. Practices, techniques, tools A meme in blogs on this issue (with Atlassian an exemplar of this) is to emphasise DevOps as \"cultural\" or \"philosophy\" in contrast to specific technical practices. Again, with a systems mindset, is is both -- just different things to different people at different levels of detail, but nobody is \"wrong\". This is why I am keen to follow a definition that loosely allows for anything so long as the goal of speeding up delivery is achieved. Do you practise Lean or Kanban to encourage small, incremental batches of change to go all the way to production as often as possible? That's DevOps. Do you employ techniques such as feature flags to make it easier to makes changes even more incremental? Then that's DevOps too. Do you make use of infrastructure as code tooling such as Terraform or use Docker to make it easier to keep development and production versions of code the same? Also DevOps. Note that we can acknowledge that engineers might consider the tools \"their\" way of viewing DevOps, but there's a definition that suits product managers too. Everyone is equally correct to use their definition as they all fit under the umbrella of things that make us go faster. Software Changes This definition stops short of saying features, preferring a looser term of \"software changes\". What does that encompass? A non-exhaustive list for consideration is: New features for users rolled out to a website New features to a library released to be used by other software applications Bug fixes in any of existing features in websites, libraries, applications Reconfiguration of production systems, such as increasing memory on a server that is struggling Changes to text copy","tags":"DevOps","url":"https://avengerpenguin.com/what-is-devops/","loc":"https://avengerpenguin.com/what-is-devops/"},{"title":"What we control and what we don't","text":"A core tenet of Stoic philosophy is accepting there are many things you cannot control, but you are not powerless because you can control your opinions about things outside of your control. This is clearly stated by the supporting quotation from Epictetus in Enchiridion Chapter 1: Some things are in our control and others not. Things in our control are opinion, pursuit, desire, aversion, and, in a word, whatever are our own actions. Things not in our control are body, property, reputation, command, and, in one word, whatever are not our own actions. -- Epictetus, Enchiridion , Chapter 1 The key insight from today -- one that I might have missed when I first read The Daily Stoic -- is what it is telling you about what you can control. The focus for today is not the difficult task of trying hard to accept what you cannot control (and therefore try to \"let go\" of worrying about these things) but instead it is offering the opportunity to focus positively on what you can control about a situation: your opinion about it. If you despair at the state of the world or politics or some disaster happening in the world. You can choose the opinion of being angry or upset about it or you can choose to acknowledge that while it is bad, it is out of your control. Or is it? It could be argued that this approach stops short. It does not acknowledge there are perhaps other thing you can control. Another quotation that came to mind on reading today's thoughts was: It is better to light a candle than to curse the darkness. -- W. L. Watkinson That is, can we some way to do good in even the darkest of situations or events? If something is not as you would like it to be, but it seems to be out of your control, maybe there is something you can still do about it. This attitude seems perfectly within the ethos of Stoicism too: do not focus on anger or other emotions about something (i.e. \"cursing\" the darkness) but instead act on what you can control in line with virtue and doing what is right. Dismayed by the state of politics? Join a political party, campaign or even just write your thoughts for others to read (making a well-articulated case just the great orators of Ancient Greece and Rome, of course, rather than getting into passionate, fruitless arguments on social media). Frustrated by your body and health? For many people there's small habits you can adopt daily to nudge your health in the right direction if only in small ways. Will a single blog in the noise change policitians' minds or steer public discourse? No. Will eating better help chronic or terminal illnesses? No. But in the darkness, it is our Stoic duty to light that candle if only to focus our discipline away from cursing it.","tags":"Stoicism","url":"https://avengerpenguin.com/what-we-control-and-what-we-dont/","loc":"https://avengerpenguin.com/what-we-control-and-what-we-dont/"},{"title":"Why Programming Language Matters","text":"Choice of programming language can be a source of religious wars between software engineers and developers. It is one of the many areas where people build a subjective opinion based on how \"nice\" or \"elegant\" something is and whether they get joy working with it or whether they find themselves fighting the language or tool every step of the way. Clearly, this is going to vary person-to-person as people's mental models vary, as their idea of elegance varies and whether a technology differs greatly from other tools with which the developer has a lot of experience. Due to the strong influence of subjectively, it is common for people to put the programming language debate into the same camp as arguing over tabs versus spaces for indentation or reoccurring arguments around editors. All programming languages can do the same things, but vary in their approach, so some feel it is a waste of time trying to argue as if there are objective measures of quality. I recently saw a strong example of this stance in the opening session of QCon London 2018 . Someone running a track on using various languages in the back end asked the room if anyone felt that choice of language mattered in back end applications and only minority of people raised their hand who were then told they were \"wrong\" if they felt it mattered. I am someone who raised their hand and I would like to explain why it is not \"wrong\" to care about choice of programming language in our applications. This is not meant to be a counter-argument or strong disagreement though. My intention here is to expand on a deliberately blanket statement like \"it doesn't matter\" as I think there is much value in exploring the nuances on where that statement is true and where it is false. Firstly, I want to acknowledge the things that absolutely do not matter (most of the time) so I do not give the impression I am defending religious or flame wars. Then I want to remind the reasons why subjectivity is important at times before finally unpacking what I think are truly objective things that matter about a programming language. Things that do not matter I fully understand people who take the \"it doesn't matter\" stance since most debates or discussion on language choice focus on trivial aspects, things that people just do not \"like\" and other subjective things. Whitespace Yes, Python uses whitespace semantically and other languages do not. No, it does not really matter. I personally like it as it removes the redundancy of using braces, for example, to determine scope and then also indenting for readability ‚Äî i.e. when I indent I achieve both scope and readability at once. If you do not like it, then that's fine too and it doesn't affect your application at the end of the day if you just get used to it or have appropriate tooling. Boilerplate Yes, Java is verbose and Scala is not. Does it really matter for your project? Only if that boilerplate soaks up real time, which is rare with appropriate tooling. Personally, I prefer to get going more quickly with Python but that's because I like to do rapid prototyping where I think there are some objective arguments that you can move faster in more expressive languages. However, in that case you are probably always better prototyping in languages you know well, which makes this a subjective point again. Typing (sort of) With appropriate testing, you do get away with achieving the same results with or without static or dynamic typing. There are debates around bug rates and I have even personally seen bugs in Node.js applications that any type checking would have trivially caught. I'll leave it as an exercise for the reader to determine if there is objective evidence around type checking and bugs. For now, I'll assume this to be a subjective matter for the purposes of my argument. Performance (sometimes‚Ä¶) It was common maybe 10 years ago to hear lots of arguments that Python was demonstrably slower than Java (or other languages) in a few cases. I've not been following performance benchmarks recently, so maybe the argument went away because Python is faster now. However, I heard a very convincing argument from a speaker at PyCon UK around that time in that Python (in her view) saves developer time ‚Äî which is expensive ‚Äî while trading off CPU time, which is arguably a lot cheaper than developer salaries now. So, while you could choose a language for optimal performance on the back end of a web application, you might be delaying delivery (which is what you get paid for) for the sake of performance tuning. With the modern landscape of cloud computing, autoscaling and serverless architectures we can paper over any need to micro-manage performance with aching, horizontal scaling or breaking up monolithic services . Of course, I am not advocating for throwing performance out the window, but we can obsess too much about milliseconds here and there when really we're paid to deliver value not necessarily on CPU cycle efficiency. I hope it goes without saying that there are clearly embedded use cases where high efficiency is key, but my focus here is on enterprise and web applications. Sometimes subjective things do matter I know this is not necessarily the intent when someone states that programming language choice \"doesn't matter\", but it does avoid the human side of the issue in that developers' happiness is important too. I think the assertion that all programming languages are equally capable for a web back end comes from a very technical viewpoint where, yes, a Ruby service could be rewritten in Rust and would work equally well. So, technically, yes, language does not matter if all that matters to you is the technical capability. Software engineering teams are made up of people who will have personal feelings about languages, who will have various levels of experience with languages and might even have various levels of general software development experience. There's a reason Python gets suggested to new programmers. If a team is about to work on a new project, then factoring in people's knowledge, opinions and, yes, even feelings is not wholly irrational. In this context, it is objectively the case that a team whose subjective needs are met will deliver the project sooner than a team who is forced to work with tools they are unfamiliar with. I realise that means I could be interpreted as supporting the argument that \"PHP is good because so many things are written in it and it's easy to deploy.\" I do not feel good about this and can only apologise. Objective differences that do matter Does all this mean that there is no right choice of programming language for a given application? Is it just about choosing something you like or are some tools a better fit for certain jobs? The argument gets thrown around that all languages are equally capable. They are, after all, all Turing Complete (or we would not likely call them programming languages at all) and for every functional solution to a problem, you can rewrite it in an object-oriented or imperative way. We know this is the case as there are whole collections of solutions to things like the Towers of Hanoi in multiple languages. So, in this sense, all languages are capable of solving (nearly) all problems, but are some objectively better at certain problems? What things make a language better? A language is more than just a language As with the subjective differences, I do actually agree the choice \"doesn't matter\" in the very technical sense that you could build anything in any language (assuming you had people that knew it well or time for them to learn). And again I disagree with that viewpoint on the basis that it's missing a wider context that is important for actual engineering of business systems and web applications. The key piece missing here is considering a language in terms of its syntax and semantics only, but in the real, applied discipline of software engineering we do more than just write code. We also build and deploy that code. We pull in dependencies, write tests and build on top of frameworks. It is important to consider the effectiveness of a language not just in terms of the language, but the ecosystem and community built up around it. Use cases where one language dominates Consider how many people got into Python because they wanted to use Jupyter , Numpy or Django . Was it not just as good to use PHP to get into data science? I mean, you could implement many machine learning algorithms yourself in PHP or JavaScript, but why would you if you wanted to see results right away? I personally only learnt Ruby because of Cucumber and Capybara . I learnt some Scala because of Apache Camel and Groovy due to Jenkins pipelines . Some of these can be done in other languages, but there is a clear \"right choice\" for many of these applications. It is this capability in the available libraries and frameworks that I think is one of the strongest objective considerations around language choice. It doesn't have to be the only consideration as you can still use less mature libraries in a language your team is familiar with if you understand what you're trading off. A lot of experimental projects I do deal with Linked Data and the Semantic Web for which I find RDFLib ‚Äî written in Python ‚Äî to be extremely powerful. It is useful for building applications that deal with RDF which much of the details around the various RDF formats abstracted away. In fact, I am yet to find an equivalent in any other language where a common Graph model can be parsed out of various formats. Most implementations elsewhere only deal with one format at a time, so I always come back to Python when I want to move quickly and not write objectively more boilerplate code. Alternatives can be less mature I should expand a bit more on the Cucumber and Apache Camel examples as someone is bound to point out that CucumberJVM and Cucumber.js exist. Also, there are other behaviour testing frameworks such as Lettuce , Nightwatch and Behat . Apache Camel is just one implementation of message routing and there are other ways to solve some of the problems it offers to solve. However, it is this exact trap I want to highlight: a lot of the time the tool in your preferred language could be less mature or playing catch-up to the more established player. This might be ok for your use case as the core features are enough for you or the familiarity of the language is a strong enough consideration that you are willing to take on the extra cost of implementing missing functionality, but so long as you understand the choice you have made. I have anecdotally seen too many people reach for alternatives to Cucumber simply because they \"don't like\" Ruby, but without even a moment's consideration of the sheer maturity of the ecosystem around Cucumber, Capybara and RSpec. In this case, they are allowing a subjective choice trump objective considerations and sometimes twist the \"language doesn't matter\" argument to support going with their preferred choice. Node.js has matured recently and big, well-known frameworks like Express.js have a lot of traction, but I have see these technologies adopted in large, high-traffic enterprise systems back when they were far less mature. I struggled with memory management in Node.js as if I were dealing with the JVM in the 90s and encountered really basic bugs like lack of connecting pooling or socket reuse in various NPM libraries. I still struggle to find good library support for Cache-Control and Vary headers in HTTP client libraries and spend objectively more time on things I simply do not think about when I do the same tasks in a Java or Python project. All of these are solvable, but it felt like developers who like JavaScript used this subjective opinion to choose a back end implementation that objectively soaked up more time when it came to the DevOps side of actually deploying the application. Like I say, there is nothing wrong with using what your team knows well, but I have rarely seen acknowledgement that there were objective advantages of other technologies. Communities matter One thing that I think has really helped Node.js mature is having more of a community form in that area. Having conferences dedicated to the technology and online discussion areas really makes a language better to work with in the enterprise. Most enterprise engineers will rely heavily on Github issues, Stack Overflow and other online forums to do their jobs. If we encounter errors, we need to be able to put the error message into Google. If we are trying to make two technologies work together, we can find complete solutions in blogs on how to configure them (e.g. if you search for \"nginx node.js\" you get countless examples of configuration you can lift to set nginx up as a reverse proxy in front of a Node.js app). The help is only there if a community reaches a critical mass such that 1) they have probably encountered most of your problems before you and 2) a culture of sharing, blogging and helping out learners can emerge. I think only certain kinds of enterprises (perhaps brave ones!) are really in a position to adopt newer things like Go or Rust before the community is really established. There's a reason the JVM is still a safe choice for the more cautious enterprise. I'm not saying bravery is bad and I think many enterprises could do with being more experimental and forward-thinking at times, but everything in moderation or you won't actually deliver anything. One of the things that keeps bring me back to Python is the community. I can see newer users coming to the language for data science or as children and there is a culture of nurturing and helping inexperienced users. Programming language matters if you to do more than just \"program\" I have played with semantics a bit here, but I think it's an important message. Programming language doesn't matter if all you are doing is programming -- i.e. just writing code -- but software engineering is more than that. As engineers, we are solving problems through code. We might want to reuse solved problems in the form of libraries or we might want to draw upon frameworks that abstract away lower-level concerns so we can focus on the higher-level problems. We want to create good teams that solve problems more quickly and more effectively that one engineer can do alone. Choosing a programming language will affect your ability to solve these problems. Choosing different languages will have different effects on your teams. If you are truly a software engineer, I think it does matter to consider your language carefully.","tags":"Software Engineering","url":"https://avengerpenguin.com/why-programming-language-matters/","loc":"https://avengerpenguin.com/why-programming-language-matters/"},{"title":"YAGNI","text":"You Ain't Gonna Need It","tags":"misc","url":"https://avengerpenguin.com/yagni/","loc":"https://avengerpenguin.com/yagni/"},{"title":"Your Team as a Distributed System","text":"A software engineering team meets the definition of a distributed system: Multiple processes (people) Inter-process communication (interpersonal communication) Disjoint address space (people have their own ideas of what they're working on) Collective goal (we hope) Applying Fallacies of Distributed Systems Applying Fallacies of distributed computing - Wikipedia to teams: 1. The network is reliable Messages between people get lost. Not every email or Slack message is received or read. Even things you say verbally might not get heard. We can compensate for this by being more \"TCP\" than \"UDP\". That is, ensure things are acknowledged with more back and forth. 2. Latency is zero Things get blocked while you're waiting for someone else to do something. We can minimise this by processes for raising if there's deadlock or ensure we deal with conflicting work. 3. Bandwidth is infinite There is only a finite amount of information we can give to another person. Face to face communicates more than a phone call where body language gone. Email is even worse. Then when there's gaps in what is communicated, people fill it in with inference and their own assumptions. We have to understand the trade-offs with low bandwidth communications. We need to understand when to upgrade to face-to-face (i.e. when we need to build up context). 4. Information is transmitted accurately Twist on \"The network is secure\". We can't assume that information we have sent is received accurately. Firstly, we need to start with the base assumption that everyone is trying their best such things are dealt with well when miscommuniation happens. We need to ensure what we heard is what was said and what we said is actually heard. In networks we use checksums for this. For people we might do active listening. 5. Topology doesn't change Organisation structures can change any time. Don't rely on it staying the same. 6. There is one administrator We can't assume there is one owner or stakeholder for something. We need to focus on consensus-building instead, which is notoriously hard for computer systems to solve. 7. Transport cost is zero Communications have a cost. The \"handshake\" problem highlights the exponential growth of point-to-point communication. 8. The network is homogeneous More diverse teams have a higher cumulative intelligence, but people's differences can lead to misunderstandings. Single Points of Failure A lot of teams end up with the lead being a single point of failure. Could be a principal engineer or just a really skilled developer. We need to apply patterns from carving up monoliths and ensure teams don't have these single individuals that don't scale. Rockstar developers don't scale. They need to more like Jazz musicians who listen to each other when others are doing their solos. How do we scale teams? Monitoring We need to monitor our teams. Does the team know where we're going? Can they articulate why they are doing what they're doing? The best tool is the 1:1 meeting to check people's understandings. Conflict Resolution We need to get people back on the same page when things get weird. This is like long-lived branches that, if left too long, become near impossible to merge back into the mainline. Communication People need to make good decisions independently. This requires good communications of what we're doing and why. Redundant Communication It's better to repeat yourself than under-communicate. Clarify of roles We need to understand other people's goals. We need to be able to express our goals. Re-iterate Collective Goals Keep reminding people of the goals. Culture eats strategy for breakfast Culture is ultimately the driving force for how you scale. Culture is the collective character of a group. It emerges from choices and decisions we make. It's who we hire, who we fire, the behaviours we reward and the behaviours we reprimand.","tags":"Software Engineering","url":"https://avengerpenguin.com/your-team-as-a-distributed-system/","loc":"https://avengerpenguin.com/your-team-as-a-distributed-system/"},{"title":"Your application should not know if it's production","text":"TODO","tags":"Software Engineering","url":"https://avengerpenguin.com/your-application-should-not-know-if-its-production/","loc":"https://avengerpenguin.com/your-application-should-not-know-if-its-production/"},{"title":"Ross Fenning","text":"Writes his notes, thoughts and opinions in this Digital Garden . He is a Lead Software Engineer at Inrix, amateur computer scientist, armchair philosopher , student of Skotokan Karate, infrequent bodhr√°n and tin whistle player and a rebel against the Oxford Comma . See rossfenning.co.uk for his professional profile and CV. You might also be interested in Things I Have Made . Read what he is doing at the moment . DevOps and Engineering What is DevOps? More DevOps topics Undifferentiated Heavy Lifting Tech Radar Keeping up with JavaScript Why Programming Language Matters Language Semantic Arguments Productivity Four Burners Theory ENTP Productivity Other Productivity thoughts","tags":"misc","url":".","loc":"."},{"title":"Topic Index","text":"Topics This is a full list of topics and areas so all notes can be seen \"at a glance\". This is to create for myself a launch point for everything and incidentally helps search crawlers. It's not necessarily intended for others to browse. Technology Software Engineering Why Programming Language Matters Keeping up with JavaScript The best software has the fewest moving parts The Open Source Hive Mind is smarter than you Software is a service industry not a manufacturing industry Every line of code is a liability Build software as if it's being deleted Humans are part of our systems Software and Subjectivity REST , RDF, Linked Data H Factor It's either HATEOAS or it's RPC DevOps Undifferentiated Heavy Lifting Platform Engineering Philosophy Stoicism Impermanence What we control and what we don't Less is More Politics There is no such thing as the individual Productivity Only work on one thing at a time","tags":"misc","url":"https://avengerpenguin.com/index","loc":"https://avengerpenguin.com/index"},{"title":"curry-howard.lagda","text":"data Either (A : Set) (B : Set) : Set where left : A ‚Üí Either A B right : B ‚Üí Either A B cases : {A B C : Set} ‚Üí Either A B ‚Üí (A ‚Üí C) ‚Üí (B ‚Üí C) ‚Üí C cases (left a) f _ = f a cases (right b) _ g = g b data ‚ä§ : Set where tt : ‚ä§ data ‚ä• : Set where -- no constructors data _√ó_ (A B : Set) : Set where _,_ : A ‚Üí B ‚Üí A √ó B infixr 4 _,_ fst : {A B : Set} ‚Üí A √ó B ‚Üí A fst (x , y) = x snd : {A B : Set} ‚Üí A √ó B ‚Üí B snd (x , y) = y proof1 : {A B : Set} ‚Üí A ‚Üí (B ‚Üí A) proof1 a b = a proof2 : {A : Set} ‚Üí (A √ó ‚ä§) ‚Üí Either A ‚ä• proof2 (a , b) = left a proof3 : {A B C : Set} ‚Üí (A ‚Üí (B ‚Üí C)) ‚Üí ((A √ó B) ‚Üí C) proof3 f (a , b) = f a b proof4 : {A B C : Set} ‚Üí (A √ó (Either B C)) ‚Üí Either (A √ó B) (A √ó C) proof4 (a , left b) = left (a , b) proof4 (a , right c) = right (a , c) proof5 : {A B C D : Set} ‚Üí ((A ‚Üí C) √ó (B ‚Üí D)) ‚Üí ((A √ó B) ‚Üí (C √ó D)) proof5 (f , g) (a , b) = (f a) , (g b) fun : {P : Set} ‚Üí (Either P (P ‚Üí ‚ä•) ‚Üí ‚ä•) ‚Üí ‚ä• fun f = f (right (Œª x ‚Üí f (left x)))","tags":"Agda","url":"https://avengerpenguin.com/curry-howardlagda/","loc":"https://avengerpenguin.com/curry-howardlagda/"},{"title":"dependent.lagda","text":"data Nat : Set where zero : Nat suc : Nat ‚Üí Nat data Vec (A : Set) : Nat ‚Üí Set where [] : Vec A zero _::_ : {n : Nat} ‚Üí A ‚Üí Vec A n ‚Üí Vec A (suc n) infixr 5 _::_ Down from: downFrom : (n : Nat) ‚Üí Vec Nat n downFrom zero = [] downFrom (suc n) = n :: downFrom n Tail: head : {A : Set}{n : Nat} ‚Üí Vec A (suc n) ‚Üí A head (x :: xs) = x tail : {A : Set}{n : Nat} ‚Üí Vec A (suc n) ‚Üí Vec A n tail (x :: xs) = xs Dot product: _+_ : Nat ‚Üí Nat ‚Üí Nat zero + y = y (suc x) + y = suc (x + y) _*_ : Nat ‚Üí Nat ‚Üí Nat zero * _ = zero (suc x) * y = y + (x * y) dotProduct : {n : Nat} ‚Üí Vec Nat n ‚Üí Vec Nat n ‚Üí Nat dotProduct [] [] = zero dotProduct (x :: xs) (y :: ys) = (x * y) + dotProduct xs ys data Fin : Nat ‚Üí Set where zero : {n : Nat} ‚Üí Fin (suc n) suc : {n : Nat} ‚Üí Fin n ‚Üí Fin (suc n) lookupVec : {A : Set} {n : Nat} ‚Üí Vec A n ‚Üí Fin n ‚Üí A lookupVec (x :: xs) zero = x lookupVec (x :: xs) (suc i) = lookupVec xs i putVec : {A : Set}{n : Nat} ‚Üí Fin n ‚Üí A ‚Üí Vec A n ‚Üí Vec A n putVec zero y (x :: xs) = y :: xs putVec (suc i) y (x :: xs) = x :: putVec i y xs Pair: data Œ£ (A : Set) (B : A ‚Üí Set) : Set where _,_ : (x : A) ‚Üí B x ‚Üí Œ£ A B data _√ó_ (A B : Set) : Set where _,_ : A ‚Üí B ‚Üí A √ó B infixr 4 _,_ _√ó'_ : (A B : Set) ‚Üí Set A √ó' B = Œ£ A (Œª _ ‚Üí B) to : {A B : Set } -> (A √ó B) ‚Üí (A √ó' B) to (a , b) = a , b from : {A B : Set } -> (A √ó' B) ‚Üí (A √ó B) from (a , b) = a , b data List (A : Set) : Set where [] : List A _::_ : A ‚Üí List A ‚Üí List A List' : (A : Set) ‚Üí Set List' A = Œ£ Nat (Vec A) fstŒ£ : {A : Set}{B : A ‚Üí Set} ‚Üí Œ£ A B ‚Üí A fstŒ£ (x , y) = x sndŒ£ : {A : Set}{B : A ‚Üí Set} ‚Üí (z : Œ£ A B) ‚Üí B (fstŒ£ z) sndŒ£ (x , y) = y []' : {A : Set} ‚Üí List' A []' = (zero), [] _::'_ : {A : Set} ‚Üí A ‚Üí List' A ‚Üí List' A x ::' xs = (suc (fstŒ£ xs)) , x :: sndŒ£ xs length : {A : Set} ‚Üí List A ‚Üí Nat length [] = zero length (x :: xs) = suc (length xs) to2 : {A : Set} ‚Üí List A ‚Üí List' A to2 [] = zero , [] to2 (x :: xs) = suc (fstŒ£ (to2 xs)) , x :: sndŒ£ (to2 xs) from2 : {A : Set} ‚Üí List' A ‚Üí List A from2 (zero , []) = [] from2 ((suc n) , x :: xs) = x :: from2 (n , xs)","tags":"Agda","url":"https://avengerpenguin.com/dependentlagda/","loc":"https://avengerpenguin.com/dependentlagda/"},{"title":"equational-reasoning.lagda","text":"data _‚â°_ {A : Set} : A ‚Üí A ‚Üí Set where refl : {x : A} ‚Üí x ‚â° x infix 4 _‚â°_ -- symmetry of equality sym : {A : Set} {x y : A} ‚Üí x ‚â° y ‚Üí y ‚â° x sym refl = refl -- transitivity of equality trans : {A : Set} {x y z : A} ‚Üí x ‚â° y ‚Üí y ‚â° z ‚Üí x ‚â° z trans refl refl = refl -- congruence of equality cong : {A B : Set} {x y : A} ‚Üí (f : A ‚Üí B) ‚Üí x ‚â° y ‚Üí f x ‚â° f y cong f refl = refl data Nat : Set where zero : Nat suc : Nat ‚Üí Nat _+_ : Nat ‚Üí Nat ‚Üí Nat zero + y = y (suc x) + y = suc (x + y) begin_ : {A : Set} ‚Üí {x y : A} ‚Üí x ‚â° y ‚Üí x ‚â° y begin p = p _end : {A : Set} ‚Üí (x : A) ‚Üí x ‚â° x x end = refl _=‚ü®_‚ü©_ : {A : Set} ‚Üí (x : A) ‚Üí {y z : A} ‚Üí x ‚â° y ‚Üí y ‚â° z ‚Üí x ‚â° z x =‚ü® p ‚ü© q = trans p q _=‚ü®‚ü©_ : {A : Set} ‚Üí (x : A) ‚Üí {y : A} ‚Üí x ‚â° y ‚Üí x ‚â° y x =‚ü®‚ü© q = x =‚ü® refl ‚ü© q infix 1 begin_ infix 3 _end infixr 2 _=‚ü®_‚ü©_ infixr 2 _=‚ü®‚ü©_ proof1 : (m n : Nat) ‚Üí m + suc n ‚â° suc (m + n) proof1 zero zero = refl proof1 zero (suc n) = refl proof1 (suc k) n = begin (suc k + suc n) =‚ü® cong suc (proof1 k n) ‚ü© suc (suc k + n) end data List (A : Set) : Set where [] : List A _::_ : A ‚Üí List A ‚Üí List A length : {A : Set} ‚Üí List A ‚Üí Nat length [] = zero length (x :: xs) = suc (length xs) replicate : {A : Set} ‚Üí Nat ‚Üí A ‚Üí List A replicate zero x = [] replicate (suc n) x = x :: replicate n x proof2 : {A : Set} ‚Üí (n : Nat) ‚Üí (x : List A) ‚Üí length (replicate n x) ‚â° n proof2 zero x = refl proof2 (suc n) [] = begin suc (length (replicate n [])) =‚ü® cong suc (proof2 n []) ‚ü© suc n end proof2 (suc n) (x :: xs) = begin length (replicate (suc n) (x :: xs)) =‚ü® cong suc (proof2 n (x :: xs)) ‚ü© suc n end","tags":"Agda","url":"https://avengerpenguin.com/equational-reasoninglagda/","loc":"https://avengerpenguin.com/equational-reasoninglagda/"},{"title":"intro.lagda","text":"Exercises from section 1 of https://raw.githubusercontent.com/jespercockx/agda-lecture-notes/master/agda.pdf data Nat : Set where zero : Nat suc : Nat ‚Üí Nat halve : Nat ‚Üí Nat halve zero = zero halve (suc zero) = zero halve (suc (suc n)) = suc (halve n) {-# BUILTIN NATURAL Nat #-} _+_ : Nat ‚Üí Nat ‚Üí Nat zero + y = y (suc x) + y = suc (x + y) Multiplication: _*_ : Nat ‚Üí Nat ‚Üí Nat zero * _ = zero (suc x) * y = y + (x * y) Logic: data Bool : Set where false : Bool true : Bool not : Bool ‚Üí Bool not false = true not true = false _&&_ : Bool ‚Üí Bool ‚Üí Bool false && y = false true && y = y _||_ : Bool ‚Üí Bool ‚Üí Bool false || y = y true || y = true Lists: data List (A : Set) : Set where [] : List A _::_ : A ‚Üí List A ‚Üí List A length : {A : Set} ‚Üí List A ‚Üí Nat length [] = zero length (x :: xs) = suc (length xs) _++_ : {A : Set} ‚Üí List A ‚Üí List A ‚Üí List A [] ++ ys = ys (x :: xs) ++ ys = x :: (xs ++ ys) map : {A B : Set} ‚Üí (A ‚Üí B) ‚Üí List A ‚Üí List B map _ [] = [] map f (x :: xs) = (f x) :: map f xs Maybe: data Maybe (A : Set) : Set where nothing : Maybe A just : A -> Maybe A lookup : {A : Set} ‚Üí List A ‚Üí Nat ‚Üí Maybe A lookup [] _ = nothing lookup (x :: xs) zero = just x lookup (x :: xs) (suc n) = lookup xs n","tags":"Agda","url":"https://avengerpenguin.com/introlagda/","loc":"https://avengerpenguin.com/introlagda/"},{"title":"partial.lagda","text":"Proof of associativity of partial functions: open import Data.Maybe using (Maybe ; just ; nothing) import Data.Maybe as Maybe open import Level renaming (zero to lzero) open import Function using (const) renaming (_‚àò_ to _‚àò·∂†_) open import Data.Fin using (Fin) renaming (zero to fz ; suc to fs) open import Data.Empty open import Relation.Nullary open import Data.Unit using (‚ä§ ; tt) open import Data.Bool open import Data.Sum as Sum open import Data.Maybe using (Maybe ; just ; nothing) import Data.Maybe as Maybe open import Category.Monad open import Relation.Binary open import Relation.Binary.Core open import Relation.Binary.PropositionalEquality using ([_] ; inspect ; sym ; cong) _‚áÄ_ : ‚àÄ {‚Ñì} ‚Üí Set ‚Ñì ‚Üí Set ‚Ñì ‚Üí Set ‚Ñì A ‚áÄ B = A ‚Üí Maybe B Subset : ‚àÄ {‚Ñì} ‚Üí Set ‚Ñì -> Set ‚Ñì Subset A = A ‚Üí Maybe A dom : ‚àÄ {‚Ñì} ‚Üí {A B : Set ‚Ñì} ‚Üí (A ‚áÄ B) ‚Üí Subset A dom f a with f a ... | just x = just a ... | nothing = nothing _‚àô_ : ‚àÄ {‚Ñì} ‚Üí {A B C : Set ‚Ñì} ‚Üí (B ‚áÄ C) ‚Üí (A ‚áÄ B) ‚Üí (A ‚áÄ C) (g ‚àô f) a with f a ... | just b = g b ... | nothing = nothing infixr 9 _‚àô_ data _‚â°_ {A : Set} : A ‚Üí A ‚Üí Set where refl : {x : A} ‚Üí x ‚â° x infix 4 _‚â°_ _‚âà_ : {A B : Set} ‚Üí Rel (A ‚áÄ B) lzero f ‚âà g = ‚àÄ a ‚Üí f a ‚â° g a infix 0 _‚âà_ ‚àô-assoc : {A B C D : Set} (f : A ‚áÄ B) (g : B ‚áÄ C) (h : C ‚áÄ D) ‚Üí (h ‚àô g) ‚àô f ‚âà h ‚àô (g ‚àô f) ‚àô-assoc f g _ a with f a ... | nothing = refl ... | just b with g b ... | nothing = refl ... | just x = refl","tags":"Agda","url":"https://avengerpenguin.com/partiallagda/","loc":"https://avengerpenguin.com/partiallagda/"},{"title":"Why can't I call my API RESTful?","text":"Semantic debates are fun, aren't they? I can build an API and call it a REST API, then someone can say it isn't truly REST and now we're back to arguing about what is and isn't REST. The most common point of contention is whether or not to follow the HATEOAS constraint. HATEOAS For those not familiar, HATEOAS is one part of the \"Uniform Interface\" constraint. Let's look at those constraints in full for context: Identification of resources -- This basically amounts to using URLs for each resource as described above (assuming we're using HTTP as the protocol, of course...) Manipulation of resources through representations -- Send a resource represented in e.g. an XML or JSON format of some kind and that should be enough for a client to read or update the resource. Self-descriptive messages -- Use headers like Content-Type to make it clear how to parse responses. Hypermedia as the engine of application state (HATEOAS) -- A client should only be able to move between resources by following hypermedia links and controls on a particular representation. The definition of HATEOAS above is an attempt to summarise it in a single sentence, but if you're not familiar with it at all, it's not likely to be any clearer that that actually means but also why you might use it. In fact, Roy Fielding -- who coined REST in his famous PhD thesis -- has himself in the past attacked HTTP APIs that claim to be \"REST\" APIs when they do not. There have also been examples of disagreements on whether HATEOAS is needed for a REST API. That there is a significant level of confusion or even disagreements and arguments suggests a lack of clarity around what HATEOAS truly is and why you might use it. The argument usually boils down to people pointing that out failure to follow the REST constraint does not a REST API make. The common retort is that it is zealous or purist to ask for this constraint to be followed. A whole camp of API designers will furthermore claim the REST-minus-HATEOAS architectural style to be valid in its own right. Some even use terms such as Pragmatic REST to describe/justify/differentiate their \"non-purist\" API design. What's missing in these arguments? Something that's missing in a lot of discussion on the web about this: any concrete discussion of the properties of the thing originally built. Semantics There's two distinct fallacies that appear when a debate around a definition or semantics emerges: we talk too much about whether something truly fits a definition without actually discussing the merits of that thing ; or we dismiss opinions of those who disagree with us by incorrectly regarding a debate as being \"just semantics\". If you already have an opinion on the REST vs. \"pragmatic\" debate, you might well recognise one of those fallacies in those with whom you disagree, but be careful of falling into the other fallacy in your own views. Both of these fallacies can be seen as extreme ends of a spectrum where at one end we get sucked into pointless debates over definitions and at the other end we see no value in discussing definitions at all , which is also irrational. It is indeed the case that an argument about whether my API is RESTful or not is not going to make much progress if we only discuss back and forth as to what REST is or whether it's useful to use the word only in a purist sense or whether we can allow for \"pragmatic\" takes on the term. What's lacking here is discussing it in the context of the API I am building . Conversely, we should not be so quick to shoot down those that point out where my API does not conform to REST. It is tempting to call such a person zealous or a purist and frame any of their arguments as irrational and therefore not worth hearing by definition. The more rational, balanced position is to discuss the definition in terms of why all the REST constraints exist and also in the context of a particular problem domain being solved by a \"RESTful\" API. Here we can rationally see if the constraints not being followed would help or hinder us; we can revisit the advantages brought by lesser-used constraints such as HATEOAS. In this context, the definition is certainly useful and we are not longer arguing \"just semantics\". Does it matter? So, surely it's still my business whether I call my API RESTful or not? Well, sort of. Semantics -- as in the meaning of words -- relies at least to some extent to people loosely agreeing what words mean. If you present a software system or a technology and use certain words to describe it, then expect confusion when you use a word differently to how others use it. I'm not saying we need to be overly pedantic about words, but if you build an API and call it RESTful, expect some confusion by some who will expect to call it in line with all of the REST constraints. In some ways, I would plead with people to avoid using the term REST if they are consciously choose not to follow the architectural style fully, but I fear we may have lost control of the term by now to the point where it's hard to trust any meaning has been followed. In practical terms, it means that if someone presents an API to me and tells me it's a REST API, I still don't quite know what to expect until I see it. It is less than ideal for any word to get to a point where it doesn't actually convey information any more. This is especially true in a technical industry like software engineering. So, technically you can call your API what you want and really the debates should centre around what is and isn't useful for the problem you are trying to solve. However, it is worth being mindful of what you are intending to communicate when you use a technical term. Expect people to be confused if you follow a mutated definition of a term. I personally like to follow they hypermedia/HATEOAS constraints for RESTful APIs and in later posts I'll discuss why that is and the decoupling people are missing out on. It is also my contention at times that if you do not want the full decoupling of REST in our distributed systems, then it's questionable why you needed to use any of REST in the first place. That certainly warrants a whole other post.","tags":"Software Engineering","url":"https://avengerpenguin.com/why-cant-i-call-my-api-restful/","loc":"https://avengerpenguin.com/why-cant-i-call-my-api-restful/"}]};